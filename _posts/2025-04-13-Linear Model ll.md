---
layout: single
title: "Lecture 9 : The Linear Model II"
---

In this time, We will discuss the rest of [Linear Model](https://isopink.github.io/Linear-Model-L/). The discussion will proceed in the following three parts:

1. Review of Nonlinear transforms

2. Logistic Regression

3. Error measure of Logistic Regression

4. Gradient Descent

---

#### 1. Review of nonlinear transforms 

We studied nonlinear transformations in [Lecture 3](https://isopink.github.io/Linear-Model-L/), but we didn’t cover everything. Therefore, we would like to add a few more points about nonlinear transformations. 

![solution](/assets/images/lm_1.svg) 

When data isn’t linearly separable, we often apply a feature transform to map $x$ to $\mathcal{Z}$-space. But this increases dimensionality, raising the VC dimension and making generalization harder. So, it’s not always a good idea. Let’s look at two examples.

![solution](/assets/images/lm_2.svg) 

<br>

##### 1.1. Case 1

The first case offers two choices: accept some $E_\text{in}$ with a linear model, or transform to a higher dimension to make the error zero. 

![solution](/assets/images/lm_3.svg) 

We can clearly recognize that transform to a higher-dimension is a disaster. We cannot gerneralize it well. By accepting some $E_\text{in}$, we can generalize it well. 

<br>

##### 1.2. Case 2

![solution](/assets/images/lm_4.svg) 

The second case is definitely non separable. There is no choice but to transform to a higher-dimension. We can think of all possible quadratic curves in $\mathcal{X}$ and describe this feature transform $z=\Phi$ as: 

<br>

$$ \Phi_2(\mathbf{x}) = (1, x_1, x_2, x_1^2, x_1 x_2, x_2^2) $$

<br>

After seeing the result looks like a circle, it’s tempting to remove terms like $x_1$, $x_2$, and $x_1 x_2$ to reduce the VC dimension. But we must not — that’s data snooping. It uses future information and leads to overfitting and poor generalization.

Feature transform is powerful tool. However, with inspection of case $1$ and $2$, It is not always useful tool. The generalization woudl impossible. We have to choose $\Phi$ carefully. 

---

#### 2. Logistic Regression

The core of the linear model is the 'signal' $ s = \mathbf{w}^\mathrm{T} \mathbf{x} $. We have seen two models based on this signal, and we are now going to introduce a third. In Logistic Regression, the signal is converted into a probability through the formula $\theta(s)$:

<br> 

$$ 
\theta(s) = \frac{e^s}{1 + e^s} 
$$ 

<br> 


It is a type of sigmoid function. As $s$ increase, $theta(s)$ approaches $1$, and as $s$ decreases, $\theta(s)$ approaches $0$. Let us consider a concrete example. To predict heart attacks, a linear classifier gives only yes or no. But since risk isn't deterministic, logistic regression is better. It outputs the probability $\theta(s)$, where $s$ is a linear risk score — higher $s$ means higher risk.

Let us first look at the target we want to learn. It is a probability, say of a patient being at risk for heart attack, that depends on the input $\mathbf{x} (the characteristics of the patient). Formally, we are trying to learn the target funtion: 

<br>

$$
f(\mathbf{x}) = \mathbb{P}[y = +1 \mid \mathbf{x}]
$$

<br>

The data doesn’t give $f$ directly, but only samples from the probability $P(y \mid \mathbf{x})$, like patients with or without heart attacks. The data is in fact generated by a noisy target: 

<br>

$$
P(y \mid \mathbf{x}) =
\begin{cases}
f(\mathbf{x}) & \text{for } y = +1; \\
1 - f(\mathbf{x}) & \text{for } y = -1.
\end{cases}
$$

<br>

Our goal is to find $g(\mathbf{x}) = \theta(\mathbf{w}^\mathrm{T} \mathbf{x}) \approx f(\mathbf{x})$. 

---

#### 3. Error measure of Logistic Regression

The standard error measure $e(h(\mathbf{x}), y)$ used in logistic regression is based on the notion of likelihood; how 'likely' is it that we would get this output $y$ from the input $\mathbf{x}$ if the target distribution $P(y \mid \mathbf{x})$ was captured by our hypothesis $h(\mathbf{x})$: 

<br>

$$
P(y \mid \mathbf{x}) =
\begin{cases}
h(\mathbf{x}) & \text{for } y = +1; \\
1 - h(\mathbf{x}) & \text{for } y = -1.
\end{cases}
$$

<br>

We substitute for $h(\mathbf{x})$ by its value $\theta(\mathbf{w}^\mathrm{T} \mathbf{x})$. And use the fact that $\theta(-s) = 1 - \theta(s)$ to get: 

<br>

$$
P(y \mid \mathbf{x}) = \theta(y\, \mathbf{w}^\mathrm{T} \mathbf{x})
$$

<br>

Since the data points are independently generated, the probability of getting all the $y_n$'s in the data set from the corresponding $x_n$'s would be the product: 

<br>

$$
\prod_{n=1}^{N} P(y_n \mid \mathbf{x}_n) = \prod_{n=1}^{N} \theta(y_n\, \mathbf{w}^\mathrm{T} \mathbf{x}_n)
$$

<br>

We want to maximize this probability. Instead, We can minimize: 

<br>

$$
- \frac{1}{N} \ln \left( \prod_{n=1}^{N} \theta(y_n\, \mathbf{w}^\mathrm{T} \mathbf{x}_n) \right)
$$

<br>

$$
= \frac{1}{N} \sum_{n=1}^{N} \ln \left( \frac{1}{\theta(y_n\, \mathbf{w}^\mathrm{T} \mathbf{x}_n)} \right)
$$

<br>

The fact that we are minimizing this qunatity allows us to treat it as an 'error measure'. Substituting $\theta$, We get the final form: 

<br>

$$
E_{\text{in}}(\mathbf{w}) = \frac{1}{N} \sum_{n=1}^{N} 
\underbrace{\ln \left( 1 + e^{-y_n\, \mathbf{w}^\mathrm{T} \mathbf{x}_n} \right)}_{e(h(\mathbf{x}_n), y_n)}
$$

<br> 

The implied pointwise error measure is $e(h(\mathbf{x}_n), y_n) = \ln(1 + e^{-y_n\, \mathbf{w}^\mathrm{T} \mathbf{x}_n})$.


---

#### 4. Gradient Descent 

Unlike the $E_{\text{in}}(\mathbf{w})$ of linear regression, $E_{\text{in}}(\mathbf{w})$ of Logistic regression is not a closed - form. We cannot calculate the proper weight vector analytically. Instead, we have iterative solution. We now introduce the new algorithm, Gradient Descent. 

![solution](/assets/images/lm_5.svg)

Gradient descent is a method to minimize functions like $E_{in}(w)$, which can be viewed as an error surface. Like a ball rolling downhill, the algorithm follows the slope to reduce the error. However, it may end up in a local minimum depending on the starting point.

![solution](/assets/images/lm_6.svg) 

Here is a particular advantage for logistic regression. With cross-entropy error in logistic regression, $E_{in}(w)$ is a convex function. This guarantees a single global minimum, so gradient descent always finds it, regardless of the starting point. There are no local minima to get stuck in. Let's now determine how to roll down the $E_{\text{in}}$ surface. 

Suppose we take a small step of size $\eta$ in the direction of a unit vector $\hat{v}$. The new weights are $\mathbf{w}(0) + \eta \hat{\vec{v}}$. Since $\eta$ is small, using the Taylor expansion to first order, we compute the $\Delta E_{in}$ as: 

<br>

<br>

$$
\begin{aligned}
\Delta E_{\text{in}} 
&= E_{\text{in}}(\mathbf{w}(0) + \eta \hat{\mathbf{v}}) - E_{\text{in}}(\mathbf{w}(0)) \\ \\
&= \eta \nabla E_{\text{in}}(\mathbf{w}(0))^\top \hat{\mathbf{v}} + O(\eta^2) \\ \\
&\ge -\eta \|\nabla E_{\text{in}}(\mathbf{w}(0))\| 
\end{aligned}
$$

<br>

since $vec{v}$ is a unit vector, equality holds if and only if: 

<br>

$$
\hat{\mathbf{v}} = - \frac{\nabla E_{\text{in}}(\mathbf{w}(0))}{\|\nabla E_{\text{in}}(\mathbf{w}(0))\|}.
$$

<br>

This direction, specified by $\vec{v}$, leads to the largest decrease in $E_{\text{in}}$. Here is one more last thing to consider, the $\eta$. Consider the following figure:

![solution](/assets/images/lm_7.svg) 


A fixed step size can be inefficient: too small slows convergence, too large causes instability near the minimum. To address this, scale the step size by the gradient norm:

<br>

$$
\eta_t = \eta \|\nabla E_{\text{in}}\|
$$

<br>

This allows large steps when far from the minimum and smaller, stable steps near it. It also cancels out the normalization in the update direction, simplifying the gradient descent update, and leads to the *fixed learning rate gradient descent algorithm*. 

<br>

<div style="border:1px solid #ccc; padding:10px; border-radius:6px">
 
1. Initialize the weights at time step \( t = 0 \) to \( \mathbf{w}(0) \). <br>
2. For \( t = 0, 1, 2, \dots \) do <br>
3.  Compute the gradient \( \mathbf{g}_t = \nabla E_{\text{in}}(\mathbf{w}(t)) \). <br>
4.  Set the direction to move, \( \mathbf{v}_t = -\mathbf{g}_t \). <br>
5.  Update the weights: \( \mathbf{w}(t+1) = \mathbf{w}(t) + \eta \mathbf{v}_t \). <br>
6.  Iterate to the next step until it is time to stop. <br>
7. Return the final weights.

</div>

<br>

In the algorithm, \( \mathbf{v}_t \) is a direction that is no longer restricted to unit length. The parameter \( \eta \) (the *learning rate*) has to be specified. A typically good choice for \( \eta \) is around \( 0.1 \) (a purely practical observation). Gradient descent is a general algorithm for minimizing twice-differntiable functions. We can apply it to the logistic regresiion in-sampler error to return weights that approximately minimize: 

<br>

$$
E_{\text{in}}(\mathbf{w}) = \frac{1}{N} \sum_{n=1}^{N} \ln \left(1 + e^{-y_n \mathbf{w}^\top \mathbf{x}_n} \right).
$$

<br>

<div style="border:1px solid #ccc; padding:10px; border-radius:6px">

1. Initialize the weights at time step \( t = 0 \) to \( \mathbf{w}(0) \). <br>
2. For \( t = 0, 1, 2, \dots \) do <br>
3.  Compute the gradient  
<br>  
$$
\mathbf{g}_t = -\frac{1}{N} \sum_{n=1}^{N} \frac{y_n \mathbf{x}_n}{1 + e^{y_n \mathbf{w}(t)^\top \mathbf{x}_n}}.
$$  
<br>
4.  Set the direction to move, \( \mathbf{v}_t = -\mathbf{g}_t \). <br>
5.  Update the weights: \( \mathbf{w}(t+1) = \mathbf{w}(t) + \eta \mathbf{v}_t \). <br>
6.  Iterate to the next step until it is time to stop. <br>
7. Return the final weights \( \mathbf{w} \).

</div>

<br>



