---
layout: single
title: "Lecture 9 : The Linear Model II"
---

In this time, We will discuss the rest of [Linear Model](https://isopink.github.io/Linear-Model-L/). The discussion will proceed in the following three parts:

1. Review of Nonlinear transforms

2. Logistic Regression

3. Error measure of Logistic Regression

4. Gradient Descent

5. Stochastic Gradient Descent

---

#### 1. Review of nonlinear transforms 

We studied nonlinear transformations in [Lecture 3](https://isopink.github.io/Linear-Model-L/), but we didn’t cover everything. Therefore, we would like to add a few more points about nonlinear transformations. 

When data isn’t linearly separable, we often apply a feature transform to map $x$ to $\mathcal{Z}$-space. But this increases dimensionality, raising the VC dimension and making generalization harder. So, it’s not always a good idea. Let’s look at two examples.

![solution](/assets/images/lm_2.svg) 

<br>

##### 1.1. Case 1

First case offers two choices: accept some $E_\text{in}$ with a linear model, or transform to a higher dimension to make the error zero. 

![solution](/assets/images/lm_3.svg) 

We can clearly recognize that transform to a higher-dimension is a disaster. We cannot gerneralize it well. By accepting some $E_\text{in}$, we can generalize it well. 

<br>

##### 1.2. Case 2

![solution](/assets/images/lm_4.svg) 

The Second case is definitely non separable. There is no choice but to transform to a higher-dimension. We can think of all possible quadratic curves in $\mathcal{X}$ and describe this feature transform $z=\Phi$ as: 

<br>

$$ \Phi_2(\mathbf{x}) = (1, x_1, x_2, x_1^2, x_1 x_2, x_2^2) $$

<br>

After seeing the result looks like a circle, it’s tempting to remove terms like $x_1$, $x_2$, and $x_1 x_2$ to reduce the VC dimension. But we must not — that’s data snooping. It uses future information and leads to overfitting and poor generalization.

Feature transform is powerful tool. However, with inspection of case $1$ and $2$, It is not always useful tool. The generalization woudl impossible. We have to choose $\Phi$ carefully. 

---

#### 2. Logistic Regression

The core of the linear model is the 'signal' $ s = \mathbf{w}^\mathrm{T} \mathbf{x} $. We have seen two models based on this signal, and we are now going to introduce a third. In Logistic Regression, the signal is converted into a probability through the formula $\theta(s)$:

<br> 

$$ 
\theta(s) = \frac{e^s}{1 + e^s} 
$$ 

<br> 


It is a type of sigmoid function. As $s$ increase, $theta(s)$ approaches $1$, and as $s$ decreases, $\theta(s)$ approaches $0$. Let us consider a concrete example. To predict heart attacks, a linear classifier gives only yes or no. But since risk isn't deterministic, logistic regression is better. It outputs the probability $\theta(s)$, where $s$ is a linear risk score — higher $s$ means higher risk.

Let us first look at the target we want to learn. It is a probability, say of a patient being at risk for heart attack, that depends on the input $\mathbf{x} (the characteristics of the patient). Formally, we are trying to learn the target funtion: 

<br>

$$
f(\mathbf{x}) = \mathbb{P}[y = +1 \mid \mathbf{x}]
$$

<br>

The data doesn’t give $f$ directly, but only samples from the probability $P(y \mid \mathbf{x})$, like patients with or without heart attacks. The data is in fact generated by a noisy target: 

<br>

$$
P(y \mid \mathbf{x}) =
\begin{cases}
f(\mathbf{x}) & \text{for } y = +1; \\
1 - f(\mathbf{x}) & \text{for } y = -1.
\end{cases}
$$

<br>

Our goal is to find $g(\mathbf{x}) = \theta(\mathbf{w}^\mathrm{T} \mathbf{x}) \approx f(\mathbf{x})$. 

---

#### 3. Error measure of Logistic Regression

The standard error measure $e(h(\mathbf{x}), y)$ used in logistic regression is based on the notion of likelihood; how 'likely' is it that we would get this output $y$ from the input $\mathbf{x}$ if the target distribution $P(y \mid \mathbf{x})$ was captured by our hypothesis $h(\mathbf{x})$: 

<br>

$$
P(y \mid \mathbf{x}) =
\begin{cases}
h(\mathbf{x}) & \text{for } y = +1; \\
1 - h(\mathbf{x}) & \text{for } y = -1.
\end{cases}
$$

<br>

We substitute for $h(\mathbf{x})$ by its value $\theta(\mathbf{w}^\mathrm{T} \mathbf{x})$. And use the fact that $\theta(-s) = 1 - \theta(s)$ to get: 

<br>

$$
P(y \mid \mathbf{x}) = \theta(y\, \mathbf{w}^\mathrm{T} \mathbf{x})
$$

<br>

Since the data points are independently generated, the probability of getting all the $y_n$'s in the data set from the corresponding $x_n$'s would be the product: 

<br>

$$
\prod_{n=1}^{N} P(y_n \mid \mathbf{x}_n) = \prod_{n=1}^{N} \theta(y_n\, \mathbf{w}^\mathrm{T} \mathbf{x}_n)
$$

<br>

We want to maximize this probability. Instead, We can minimize: 

<br>

$$
- \frac{1}{N} \ln \left( \prod_{n=1}^{N} \theta(y_n\, \mathbf{w}^\mathrm{T} \mathbf{x}_n) \right)
$$

<br>

$$
= \frac{1}{N} \sum_{n=1}^{N} \ln \left( \frac{1}{\theta(y_n\, \mathbf{w}^\mathrm{T} \mathbf{x}_n)} \right)
$$

<br>

The fact that we are minimizing this qunatity allows us to treat it as an 'error measure'. Substituting $\theta$, We get the final form: 

<br>

$$
E_{\text{in}}(\mathbf{w}) = \frac{1}{N} \sum_{n=1}^{N} 
\underbrace{\ln \left( 1 + e^{-y_n\, \mathbf{w}^\mathrm{T} \mathbf{x}_n} \right)}_{e(h(\mathbf{x}_n), y_n)}
$$

<br> 

The implied pointwise error measure is $e(h(\mathbf{x}_n), y_n) = \ln(1 + e^{-y_n\, \mathbf{w}^\mathrm{T} \mathbf{x}_n})$.


---

#### 4. Gradient Descent 

Unlike the $E_{\text{in}}(\mathbf{w})$ of linear regression, $E_{\text{in}}(\mathbf{w})$ of Logistic regression is not a closed - form. We cannot calculate the proper weight vector analytically. Instead, we have iterative solution. We now introduce the new algorithm, Gradient Descent. 

Gradient Descent is a method to minimize functions like $E_{in}(w)$, which can be viewed as an error surface. Like a ball rolling downhill, the algorithm follows the slope to reduce the error. However, it may end up in a local minimum depending on the starting point.

![solution](/assets/images/lm_6.svg) 

Here is a particular advantage for logistic regression. With cross-entropy error in logistic regression, $E_{in}(w)$ is a convex function. This guarantees a single global minimum, so gradient descent always finds it, regardless of the starting point. There are no local minima to get stuck in. Let's now determine how to roll down the $E_{\text{in}}$ surface. 

Suppose we take a small step of size $\eta$ in the direction of a unit vector $\hat{mathbf{v}}$. The new weights are $\mathbf{w}(0) + \eta \hat{\vec{v}}$. Since $\eta$ is small, using the Taylor expansion to first order, we compute the $\Delta E_{in}$ as: 

<br>

$$
\begin{aligned}
\Delta E_{\text{in}} 
&= E_{\text{in}}(\mathbf{w}(0) + \eta \hat{\mathbf{v}}) - E_{\text{in}}(\mathbf{w}(0)) \\ \\
&= \eta \nabla E_{\text{in}}(\mathbf{w}(0))^\top \hat{\mathbf{v}} + O(\eta^2) \\ \\
&\ge -\eta \|\nabla E_{\text{in}}(\mathbf{w}(0))\| 
\end{aligned}
$$

<br>

since $hat{\mathbf{v}}$ is a unit vector, equality holds if and only if: 

<br>

$$
\hat{\mathbf{v}} = - \frac{\nabla E_{\text{in}}(\mathbf{w}(0))}{\|\nabla E_{\text{in}}(\mathbf{w}(0))\|}.
$$

<br>

This direction, specified by $\vec{v}$, leads to the largest decrease in $E_{\text{in}}$. Here is one more last thing to consider, the $\eta$. Consider the following figure:

![solution](/assets/images/lm_7.svg) 


A fixed step size can be inefficient: too small slows convergence, too large causes instability near the minimum. To address this, scale the step size by the gradient norm:

<br>

$$
\eta_t = \eta \|\nabla E_{\text{in}}\|
$$

<br>

This allows large steps when far from the minimum and smaller, stable steps near it. It also cancels out the normalization in the update direction, simplifying the gradient descent update, and leads to the *fixed learning rate gradient descent algorithm*. 

<br>

<div style="border:1px solid #ccc; padding:10px; border-radius:6px">
 
1. Initialize the weights at time step \( t = 0 \) to \( \mathbf{w}(0) \). <br>
2. For \( t = 0, 1, 2, \dots \) do <br>
3.  Compute the gradient \( \mathbf{g}_t = \nabla E_{\text{in}}(\mathbf{w}(t)) \). <br>
4.  Set the direction to move, \( \mathbf{v}_t = -\mathbf{g}_t \). <br>
5.  Update the weights: \( \mathbf{w}(t+1) = \mathbf{w}(t) + \eta \mathbf{v}_t \). <br>
6.  Iterate to the next step until it is time to stop. <br>
7. Return the final weights.

</div>

<br>

In the algorithm, $ \mathbf{v}_t $ is a direction that is no longer restricted to unit length. The parameter $ \eta $ (the *learning rate*) has to be specified.  
A typically good choice for $ \eta $ is around $ 0.1 $ (a purely practical observation). Gradient descent is a general algorithm for minimizing twice-differentiable functions. We can apply it to the logistic regression in-sample error to return weights that approximately minimize:

<br>

$$
E_{\text{in}}(\mathbf{w}) = \frac{1}{N} \sum_{n=1}^{N} \ln \left(1 + e^{-y_n \mathbf{w}^\top \mathbf{x}_n} \right).
$$

<br>

<div style="border:1px solid #ccc; padding:10px; border-radius:6px">

1. Initialize the weights at time step \( t = 0 \) to \( \mathbf{w}(0) \). <br>
2. For \( t = 0, 1, 2, \dots \) do <br>
3.  Compute the gradient  
<br>  
$$
\mathbf{g}_t = -\frac{1}{N} \sum_{n=1}^{N} \frac{y_n \mathbf{x}_n}{1 + e^{y_n \mathbf{w}(t)^\top \mathbf{x}_n}}.
$$  
<br>
4.  Set the direction to move, \( \mathbf{v}_t = -\mathbf{g}_t \). <br>
5.  Update the weights: \( \mathbf{w}(t+1) = \mathbf{w}(t) + \eta \mathbf{v}_t \). <br>
6.  Iterate to the next step until it is time to stop. <br>
7. Return the final weights \( \mathbf{w} \).

</div>

<br>

#### 5. Stochastic Gradient Descent

The version of gradient descent we have described so far is known as *batch* gradient descent(BGD) – the gradient is computed for the error on the whole data set before a weight update is done. A sequential version of gradient descent known as *stochastic gradient descent* (SGD) turns out to be very efficient in practice.  
Instead of considering the full batch batch gradient on all $N$ training data points, we consider a stochastic version of the gradient. 

First, Pick a random training point $(x_n, y_n)$ and update the model using only its error. This randomness is why it's called *stochastic*. And consider only the error on that data point. In the case of logistic regression, 

<br>

$$
e_n(\mathbf{w}) = \ln \left( 1 + e^{-y_n \, \mathbf{w}^\top \mathbf{x}_n} \right)
$$

<br>

and the weight updata is, 

$$
\mathbf{w(t+1)} = \mathbf{w(t)} - \eta \nabla e_n(\mathbf{w})
$$

Since the data is picked uniformly at random from $\{1, \ldots, N\}$, the expected weight change is: 

<br>

$$
\mathbb{E}_n \left[ -\nabla \mathbf{e}\left(\mathbf{w}\right) \right] = \frac{1}{N} \sum_{n=1}^{N} -\nabla \mathbf{e}\left(\mathbf{w}\right)
$$

<br>

SGD behaves like batch gradient descent on average, though it's noisier due to randomness. Over time, these fluctuations cancel out. There are some advantages of SGD. One major advantage of SGD is its faster computation. Unlike batch gradient descent, which uses all data points to compute the gradient, SGD uses only one data point per update, making each step much quicker.  

Another benefit is its randomness, which helps escape local minima. Real-world loss surfaces are often bumpy, and SGD's noisy updates give it a chance to jump out of bad spots and find better solutions. The following figure illustrates it as below: 

![solution](/assets/images/lm_8.svg) 
