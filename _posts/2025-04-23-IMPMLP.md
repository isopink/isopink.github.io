---
layout : single
title : "[Implement] Building Neural Network"
--- 

In this time, we are goind to implement Neural Network from scratch. If you are interested in theoretical backgrund, take a look at my [previous post](https://isopink.github.io/Neural-Network/). The discussion will procced in the following four parts: 

1. Activation function

2. Forward Propagation 

3. Desiginig the output layer 

4. Applying on MNIST 

---

#### 1. Activation function 

The activation function plays a key role in converting the input received by a node into an output value in a neural network. In the [previous post](https://isopink.github.io/percepn/), we implemented some basic logic gates.  The functions used there are also considered activation functions. Specifically, step functions. However, we will use sigmoid as an activation function in this post. It looks like as follows: 

<br>

$$
h(x) = \frac{1}{1 + e^{-x}}
$$

<br>

The reason I use the sigmoide as an activation function is because of the discontinuity of the step-function. This will be a tremendous obstacle to optimization through derivative. The comparison of the two is as follows. 

![solution](/assets/images/impmlp1.svg)

We can simply(very easy) implment sigmoid as below: 

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))    
```
---

#### 2. Forward Propagation

Let us implement the three-layer perceptron as shown below. I will assign some random wieghts and bias for clearity. However, most initialization begins with Gaussian... Our network is illustrated below: 

![solution](/assets/images/impmlp2.svg)

Anyway, we need to compute forward propagation. It can be coded as below:

```python

def init_network():
    network = {}
    network['W1'] = np.array([[0.1,0.3,0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
    network['b2'] = np.array([0.1,0.2])
    network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])
    network['b3'] = np.array([0.1, 0.2])
    
    return network 

def forward(network ,x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) +b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1,W2) + b2 
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3 
    y = identity_function(a3)

    return y 

network = init_network()
x = np.array([1.0,0.5])
y = forward(network, x)

print(y)
```

---

#### 3. Desiginig output layer 

We can also apply activation function to the output layer. Many cases use softmax function as an activation function. Since we are focusing on implementation, we will not talk about the important properties of the softmax function. I will post related topic later. So now, let us describe the function as below:

<br>

$$
y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}
$$

<br>

Unlike the simplicity of the function, there is one thing to worry about, the overflow. Without concerning overflow, we can code it as follows: 

```python
import numpy as np 

def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a 
    return y
```

The overflow occurs because the exponential grows too fast. A simple trick can rescue us. Before implementing, let us see modified version of softmax function as shown below: 

<br>

$$
\begin{aligned}
y_k                                &= \frac{\exp(a_k)}{\sum_{i=1}^n \exp(a_i)} \\

\\
                                   &= \frac{C \exp(a_k)}{C \sum_{i=1}^n \exp(a_i)} \\

\\
                                   &= \frac{\exp(a_k + \log C)}{\sum_{i=1}^n \exp(a_i + \log C)} \\

\\
                                   &= \frac{\exp(a_k + C')}{\sum_{i=1}^n \exp(a_i + C')}
\end{aligned}
$$


<br>

This implies that the value of softmax function does not change, even if we add arbitrary integer. In general, the constant $C$ is the maximum value of input signals. So now, let us update the code as below: 

```python
import numpy as np

def softmax(a):
    c =np.max(a)
    exp_a = np.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y 
```

It is far more safe than the previous one. Now we need to define the number of nodes in output layer. It depends on how many classes one wants to classify. We will apply this criteria on MNIST data set.

--- 

#### 4. Applying on MNIST

Since there are 10 digits, we will make 10 nodes in the output layer. We have 60,000 training data and 10,000 test data consisting of 28x28 pixels, respectively. And we will use pre-trained weights here. You can download all these files [here](address). Let us read the MNIST data first. 

<br>

##### 4.1. Load the MNIST data

To begin with, we load the MNIST dataset using a utility function `load_mnist()`. This function handles reading the compressed data files, normalizing the pixel values, and optionally flattening the images or converting labels to one-hot vectors.

```python
from mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)
```

The parameter `normalize=True` scales each pixel to the range [0.0, 1.0], and `flatten=True` reshapes the images into 1D vectors of length 784 (28×28). If `one_hot_label=True`, the labels would be returned as one-hot encoded arrays, but here we use simple integer labels for simplicity.

<br>

##### 4-2. Using pre-trained weights

Instead of training the network from scratch, we will load pre-trained weights that were already trained on the MNIST training set. These weights and biases are stored in a file named `sample_weight.pkl`. Here is how we load them:

```python
import pickle

def init_network():
    with open("sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
    return network
``` 

The returned object network is a dictionary that contains weights and biases for each layer: 'W1', 'b1', 'W2', 'b2', 'W3', and 'b3'.

<br>

##### 4.3. Predicting Labels 

Now that we have the data and the network, let's perform forward propagation on the test set and measure the model’s accuracy. We first define the predict() function, which performs forward propagation through the network and uses the softmax function to compute the final probabilities:

```python
def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)
    return y
```

Then we run the model on all 10,000 test images and calculate the accuracy:

```python
network = init_network()
accuracy_cnt = 0

for i in range(len(x_test)):
    y = predict(network, x_test[i])
    p = np.argmax(y)  # predicted label
    if p == t_test[i]:
        accuracy_cnt += 1

print("Accuracy:", float(accuracy_cnt) / len(x_test))
```

This will print the classification accuracy of the model. With properly trained weights, the accuracy should exceed 95%. 

<br>

##### 4.5. Batch Processing 

Batch processing accelerates computation by optimizing array operations and reducing transfer bottlenecks. So now we need to look at how to implement this. 

```python
(_, _), (x, t) = load_mnist(normalize=True, flatten=True, one_hot_label=False)
network = init_network()

batch_size = 100 
accuracy_cnt = 0

for i in range(0, len(x), batch_size):
    x_batch = x[i:i+batch_size]
    y_batch = predict(network, x_batch)
    p = np.argmax(y_batch, axis=1)
    accuracy_cnt += np.sum(p == t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
```

