---
layout : single
title : "[Implement] Building Neural Network"
--- 

In this time, we are goind to implement Neural Network from scratch. If you are interested in theoretical backgrund, take a look at my [previous post](https://isopink.github.io/Neural-Network/). The discussion will procced in the following four parts: 

1. Activation function

2. Forward Propagation 

3. Desiginig the output layer 

4. Applying on MNIST 

---

#### 1. Activation function 

The activation function plays a key role in converting the input received by a node into an output value in a neural network. In the [previous post](https://isopink.github.io/percepn/), we implemented some basic logic gates.  The functions used there are also considered activation functions. Specifically, step functions. However, we will use sigmoid as an activation function in this post. It looks like as follows: 

<br>

$$
h(x) = \frac{1}{1 + e^{-x}}
$$

<br>

The reason I use the sigmoide as an activation function is because of the discontinuity of the step-function. This will be a tremendous obstacle to optimization through derivative. Now, we can simply(very) implment sigmoid as below: 

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))    
```
---

#### 2. Forward Propagation

Let us implement the three-layer perceptron as shown below. I will assign some random wieghts and bias for clearity. However, most initialization begins with Gaussian... Our network is illustrated below: 

![solution](/assets/images/impmlp2.svg)

Anyway, we need to compute forward propagation. It can be coded as below:

```python

def init_network():
    network = {}
    network['W1'] = np.array([[0.1,0.3,0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
    network['b2'] = np.array([0.1,0.2])
    network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])
    network['b3'] = np.array([0.1, 0.2])
    
    return network 

def forward(network ,x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) +b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1,W2) + b2 
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3 
    y = identity_function(a3)

    return y 

network = init_network()
x = np.array([1.0,0.5])
y = forward(network, x)

print(y)
```

---

#### 3. Desiginig output layer 

We can also apply activation function to the output layer. Many cases use softmax function as an activation function. Since we are focusing on implementation, we will not talk about the important properties of the softmax function. I will post related topic later. So now, let us describe the function as below:

<br>

$$
y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}
$$

<br>

Unlike the simplicity of the function, there is one thing to worry about, the overflow. Without concerning overflow, we can code it as follows: 

```python
import numpy as np 

def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a 
    return y
```

The overflow occurs because the exponential grows too fast. A simple trick can rescue us. Before implementing, let us see modified version of softmax function as shown below: 

<br>

$$
\begin{aligned}
y_k                                &= \frac{\exp(a_k)}{\sum_{i=1}^n \exp(a_i)} \\

\\
                                   &= \frac{C \exp(a_k)}{C \sum_{i=1}^n \exp(a_i)} \\

\\
                                   &= \frac{\exp(a_k + \log C)}{\sum_{i=1}^n \exp(a_i + \log C)} \\

\\
                                   &= \frac{\exp(a_k + C')}{\sum_{i=1}^n \exp(a_i + C')}
\end{aligned}
$$


<br>

This implies that the value of softmax function does not change, even if we add arbitrary integer. In general, the constant $C$ is the maximum value of input signals. So now, let us update the code as below: 

```python
import numpy as np

def softmax(a):
    c =np.max(a)
    exp_a = np.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y 
```

It is far more safe than the previous one. Now we need to define the number of nodes in output layer. It depends on how many classes one wants to classify. We will apply this criteria on MNIST data set.

--- 

#### 4. Applying on MNIST

I hope you all already know the MNIST data, set of hand-written digits. Since there are 10 digits, we will make 10 nodes in the output layer. 
