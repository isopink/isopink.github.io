---
layout: single
title: Introduction to Deep Learning
---

### Table of Contents

1.  **Intro: The Dream of AI and the Rise of Deep Learning**
    1.  AI's Long History and Early Challenges
    2.  Deep Learning for Intuitive Problems

2.  **Early AI Steps and Limits**
    1.  Solving Formal Problems
    2.  Easy for Humans, Hard for Computers
    3.  Failure of "Hard-Coded" AI

3.  **Machine Learning Begins: The Importance of Representation**
    1.  Switch to Machine Learning
    2.  Early ML and "Features"
    3.  What is Representation Learning?
    4.  Autoencoders and Layered Info

4.  **Deep Learning: Layers of Understanding**
    1.  Building Complex Ideas from Simple Ones
    2.  Multi-Layer Perceptrons (MLPs)
    3.  Two Ways to Measure "Depth"

5.  **A Brief History of Deep Learning**
    1.  Three Big Waves
        1.  Cybernetics (1940s-1960s)
        2.  Connectionism (1980s-1990s)
        3.  Deep Learning Returns (Since 2006)

6.  **Why Deep Learning is Powerful Now**
    1.  More Data
    2.  Bigger Models
    3.  Better Accuracy, More Complex Tasks, Real-World Impact

---

### 1. Intro: The Dream of AI and the Rise of Deep Learning

People have always dreamed of making machines that can think. This idea goes way back to ancient Greece. Even a hundred years before computers were built, people wondered if these machines could become smart. Today, Artificial Intelligence (AI) is a booming field. It helps us automate tasks, understand speech and images, make medical diagnoses, and support science.

**1.1. AI's Long History and Early Challenges**
In the beginning, AI quickly solved problems that are hard for humans but easy for computers. These were problems with clear, mathematical rules.

**1.2. Deep Learning for Intuitive Problems**
[cite_start]But the real challenge for AI was solving tasks that are easy for people but hard to explain to a computer, like recognizing voices or faces in images. [cite_start]This book is about a solution to these "intuitive" problems: **deep learning**. [cite_start]Deep learning lets computers learn from experience and understand the world using layers of ideas, where simple ideas build up to complex ones. [cite_start]This way, humans don't have to tell the computer everything it needs to know.

### 2. Early AI Steps and Limits

**2.1. Solving Formal Problems**
[cite_start]Early AI successes happened in simple, rule-based settings and didn't need computers to have much world knowledge. [cite_start]For example, IBM's Deep Blue beat a chess champion in 1997. [cite_start]Chess is a very simple world with clear rules that are easy for programmers to give to a computer.

**2.2. Easy for Humans, Hard for Computers**
[cite_start]Ironically, abstract and formal tasks that are among the hardest for humans are among the easiest for computers. [cite_start]Computers could beat chess players long ago, but only recently started matching average human ability to recognize objects or speech. [cite_start]Everyday life needs a huge amount of common sense knowledge. [cite_start]Much of this is intuitive and hard to put into formal rules. [cite_start]Getting this "informal" knowledge into a computer is a key challenge for AI.

**2.3. Failure of "Hard-Coded" AI**
Some AI projects tried to hard-code knowledge using formal languages. [cite_start]This is called the **knowledge base approach** to AI[cite: 27, 29]. [cite_start]But none of these worked well[cite: 30]. [cite_start]For example, Cyc, a famous project [cite: 31][cite_start], failed to understand a simple story about shaving because it couldn't handle everyday logic; it struggled to create formal rules complex enough to describe the world accurately. [cite_start]This showed that AI systems need to learn knowledge on their own by finding patterns in raw data.

### 3. Machine Learning Begins: The Importance of Representation

**3.1. Switch to Machine Learning**
[cite_start]This ability to learn from data is called **machine learning**. [cite_start]Machine learning allowed computers to handle real-world problems and make decisions that seemed subjective. [cite_start]Simple examples include predicting if a C-section is needed (logistic regression)  [cite_start]or separating spam emails (naive Bayes).

**3.2. Early ML and "Features"**
[cite_start]But these early machine learning tools relied heavily on how data was **represented**. [cite_start]For instance, a system predicting C-sections wouldn't look at the patient directly; instead, a doctor would tell it specific "features" like "uterine scar present". [cite_start]The system learned how these features correlated with outcomes, but couldn't create them. [cite_start]If given an MRI scan (raw data) instead of a doctor's report, it couldn't make good predictions, because individual pixels don't directly relate to delivery complications. [cite_start]This problem with representations is common in computer science. [cite_start]The choice of representation has a huge effect on machine learning algorithm performance.

**3.3. What is Representation Learning?**
[cite_start]Many AI tasks can be solved by choosing the right "features" and feeding them to a simple machine learning tool. [cite_start]But for many tasks, it's hard to know what features to extract. [cite_start]For example, detecting cars in photos is tricky because a wheel's appearance can change a lot due to shadows, glare, or being partly hidden.

[cite_start]One solution is to use machine learning to discover not just how to use a representation, but the representation itself[cite: 67]. [cite_start]This is called **representation learning**[cite: 68]. [cite_start]Learned representations often work much better than human-designed ones[cite: 68]. [cite_start]They also help AI systems adapt to new tasks quickly with minimal human intervention[cite: 69]. [cite_start]Designing features for a complex task can take decades for researchers [cite: 71, 72][cite_start], but a representation learning algorithm can find good features in minutes to months.

**3.4. Autoencoders and Layered Info**
[cite_start]A classic example of a representation learning algorithm is the **autoencoder**. [cite_start]It has an "encoder" to change data into a new representation and a "decoder" to change it back. [cite_start]Autoencoders are trained to keep as much info as possible while making the new representation have good properties. [cite_start]Our goal is usually to separate the "factors of variation" (different influences) that explain the data. [cite_start]These factors are often not directly seen, but are concepts or abstractions that help us understand data. [cite_start]For example, in a speech recording, factors include age, sex, accent, and the words spoken. [cite_start]In an image of a car, factors include its position, color, and sun angle.

### 4. Deep Learning: Layers of Understanding

**4.1. Building Complex Ideas from Simple Ones**
[cite_start]A big problem in real-world AI is that many factors influence every bit of data we observe. [cite_start]A red car might look black at night, or its shape depends on the viewing angle. [cite_start]We often need to untangle these factors and ignore the irrelevant ones. [cite_start]But extracting such high-level, abstract features from raw data is very hard. [cite_start]Sometimes, getting the representation is almost as hard as solving the problem itself.

[cite_start]**Deep learning** solves this core problem in representation learning by creating representations that are built from other, simpler representations. [cite_start]Deep learning lets the computer build complex ideas from simpler ones. [cite_start]For example, a deep learning system can represent a person's image by combining simpler ideas like corners and contours, which are built from edges. [cite_start]This happens by extracting increasingly abstract features through layers (edges, corners/contours, object parts) from input pixels.

**4.2. Multi-Layer Perceptrons (MLPs)**
[cite_start]A typical deep learning model is the **feedforward deep network** or **multilayer perceptron (MLP)**. [cite_start]An MLP is just a math function that takes inputs and gives outputs. [cite_start]This function is made by putting together many simpler functions. [cite_start]Each step of applying a math function creates a new way of representing the input.

**4.3. Two Ways to Measure "Depth"**
[cite_start]One way to think about deep learning is learning the right representation for data. [cite_start]Another way is that "depth" lets the computer learn a multi-step program. [cite_start]Each layer can be seen as the computer's memory state after running a set of instructions. [cite_start]Deeper networks can run more instructions in order.

[cite_start]There are two main ways to measure a model's depth[cite: 127]. [cite_start]One is based on the number of sequential steps needed to run the model[cite: 128, 129]. [cite_start]The other, used by deep probabilistic models [cite: 132][cite_start], looks at how concepts relate to each other, not just the computation steps. [cite_start]Because it's not always clear which view is best, and different people choose different basic "elements," there's no single correct depth for an AI model. [cite_start]There's also no clear rule for how much depth makes a model "deep". [cite_start]However, deep learning is simply the study of models that combine learned functions or concepts much more than traditional machine learning.

[cite_start]To sum up, deep learning, the focus of this book, is an AI approach. [cite_start]It's a type of machine learning that lets computers get better with experience and data. [cite_start]The authors believe machine learning is the only way to build AI systems that work in complex, real-world settings. [cite_start]Deep learning is a special kind of machine learning that gains power by learning to represent the world as nested layers of concepts, with each concept built from simpler ones.

### 5. A Brief History of Deep Learning

[cite_start]Deep learning has a long and rich history, but it has been called many names and its popularity has changed.

**5.1. Three Big Waves**
[cite_start]There have been three main waves of deep learning development:

**5.1.1. Cybernetics (1940s-1960s)**
[cite_start]This first wave was called **cybernetics**[cite: 177, 195]. [cite_start]Early learning algorithms were meant to be computer models of how brains learn[cite: 178]. [cite_start]This led to deep learning also being called **artificial neural networks (ANNs)**[cite: 179]. [cite_start]Models like the McCulloch-Pitts Neuron (1943) were early examples of brain function[cite: 196]. [cite_start]This linear model could sort inputs into two types by checking if a value was positive or negative[cite: 197]. [cite_start]Human operators had to set the weights for these models[cite: 198, 199]. [cite_start]In the 1950s, the perceptron (1958, 1962) was the first model that could *learn* these weights from examples[cite: 199]. [cite_start]The adaptive linear element (ADALINE, 1960) also learned to predict real numbers from data[cite: 200]. [cite_start]These early learning algorithms heavily influenced modern machine learning[cite: 201]. [cite_start]The training method for ADALINE was an early version of **stochastic gradient descent** [cite: 202][cite_start], which is still used today for deep learning models[cite: 203]. [cite_start]These **linear models** are still widely used [cite: 205][cite_start], but they had big limits; famously, they couldn't learn the XOR function. [cite_start]This led to a big drop in neural network popularity. [cite_start]Today, neuroscience inspires deep learning but isn't the main guide.

**5.1.2. Connectionism (1980s-1990s)**
[cite_start]In the 1980s, the second wave of neural network research came through a movement called **connectionism** or parallel distributed processing. [cite_start]The main idea was that many simple computer units working together can act intelligently. [cite_start]A key concept from this time is **distributed representation**. [cite_start]This means each input should be shown by many features, and each feature should be part of many inputs. [cite_start]For example, to recognize red cars, trucks, and birds, instead of 9 separate neurons (red car, red truck, etc.), you could use 3 for color and 3 for object type, totaling 6 neurons. [cite_start]This way, the "redness" neuron learns from all red objects.

[cite_start]Another big achievement of connectionism was the successful use and popularity of **back-propagation** to train deep neural networks with internal representations[cite: 252]. [cite_start]This algorithm is still the main way to train deep models today[cite: 253]. [cite_start]In the 1990s, researchers made progress in modeling sequences with neural networks[cite: 254]. [cite_start]The **long short-term memory (LSTM)** network (1997) helped solve problems with long sequences[cite: 256]. [cite_start]Today, LSTMs are widely used for sequence tasks like natural language processing at Google[cite: 257]. [cite_start]This second wave ended in the mid-1990s[cite: 258]. [cite_start]Overly ambitious claims from AI companies disappointed investors[cite: 259, 260]. [cite_start]At the same time, other machine learning fields like kernel machines and graphical models made breakthroughs[cite: 261]. [cite_start]These factors led to a decline in neural network popularity until 2007[cite: 262]. [cite_start]During this quiet period, neural networks still performed well on some tasks [cite: 262][cite_start], and organizations like CIFAR helped keep the research alive. [cite_start]Deep networks were thought to be hard to train around 2006, likely because the algorithms were too computationally costly for the hardware available then.

**5.1.3. Deep Learning Returns (Since 2006)**
[cite_start]The third wave of neural network research started with a breakthrough in 2006. [cite_start]Geoffrey Hinton showed that a **deep belief network** could be efficiently trained using **greedy layer-wise pre-training**. [cite_start]Other groups quickly found the same strategy worked for many other deep networks. [cite_start]This wave popularized the term "**deep learning**" to highlight that researchers could now train deeper networks and focus on the importance of depth. [cite_start]Deep neural networks started outperforming other AI systems. [cite_start]This third wave is still ongoing, though the focus has shifted from new unsupervised learning to leveraging large labeled datasets for older supervised learning algorithms.

### 6. Why Deep Learning is Powerful Now

**6.1. More Data**
[cite_start]Deep learning is now crucial partly because the amount of training data has grown. [cite_start]Our society's increasing digitization means more of our activities are recorded, making it easier to create large datasets. [cite_start]This "Big Data" era has made machine learning much easier. [cite_start]As of 2016, a supervised deep learning algorithm generally performs well with about 5,000 labeled examples per category, and can match or beat human performance with at least 10 million labeled examples.

**6.2. Bigger Models**
[cite_start]Another reason for neural networks' current success is that we now have the computing power to run much larger models. [cite_start]The idea from connectionism is that animals get smart when many neurons work together. [cite_start]Artificial neural networks have doubled in size roughly every 2.4 years since hidden units were introduced. [cite_start]This growth is fueled by faster computers with more memory and the availability of larger datasets. [cite_start]Bigger networks achieve higher accuracy on more complex tasks. [cite_start]Even today's "large" networks are smaller than the nervous systems of animals like frogs.

**6.3. Better Accuracy, More Complex Tasks, Real-World Impact**
[cite_start]Since the 1980s, deep learning has steadily improved its accuracy in recognition and prediction. [cite_start]It has also been applied successfully to more and more tasks. [cite_start]Early models recognized single objects in tiny images, but modern networks process high-resolution photos and recognize thousands of categories. [cite_start]A big moment was when a convolutional network won the ImageNet challenge for the first time, drastically cutting the top-5 error rate from 26.1% to 15.3%. [cite_start]Deep learning has also greatly improved speech recognition, cutting error rates in half. [cite_start]It's shown superhuman performance in traffic sign classification and has impacted pedestrian detection and image segmentation.

[cite_start]Deep networks can now output entire sequences of characters transcribed from an image. [cite_start]Recurrent neural networks like LSTM are used for sequence-to-sequence learning, which is revolutionizing machine translation. [cite_start]This trend toward more complex tasks has even led to neural Turing machines that can learn simple programs from examples.

[cite_start]Another major success is deep learning's extension into **reinforcement learning**. [cite_start]DeepMind showed a deep learning system could learn to play Atari video games at human level. [cite_start]Deep learning has also improved reinforcement learning for robotics. [cite_start]Many of these deep learning applications are very profitable and used by top tech companies.

[cite_start]Advances in deep learning also rely on better software tools like Theano and TensorFlow. [cite_start]Deep learning has also contributed to other sciences, for example, by providing models for visual processing for neuroscientists. [cite_start]It helps predict molecular interactions for drug design, search for subatomic particles, and analyze microscope images to build 3D brain maps. [cite_start]We expect deep learning to appear in more scientific fields in the future.

[cite_start]In short, deep learning is a machine learning approach that has used knowledge from the human brain, statistics, and math to grow over decades. [cite_start]Its recent surge in popularity and usefulness is due to more powerful computers, larger datasets, and new ways to train deeper networks.
