---
layout: single
title: "Lecture 16 : Radial Basis Functions"
---

In this time, we will discuss the RBF. Through this, we will learn how to solve problems using unsupervised learning for data without labels. The discussion will proceed in following four parts: 

1. RBF and nearest neighbors

2. RBF and neural networks

3. RBF and kernel methods

4. RBF and regularization

---

#### 1. Basic RBF model 

The basic idea of Radial Basis Function is that every point in the dataset affects the hypothesis. But, this is obvious since we build the hypothesis from the data. However, RBF receives influence in a special way through distance. In other words, one point in the dataset affects other nearby points more than the ones farther away. We can formalize it as below: 

<br>

<div align="center">
Each $(\mathbf{x}_n, y_n) \in \mathcal{D}$ influences $h(\mathbf{x})$ based on $\|\mathbf{x} - \mathbf{x}_n\|$
</div>

<br>

Now, let us llo at a figure below: 

![solution](/assets/images/rbf_1.svg) 

The center of this bump is $x_n$. This shows how the influence of $x_n$ on the neigoring points in the space. So, it is most influential nearby. Since it is affected by only distance, bump looks symmetric around. Now, let us introudce the standarad form of RBF model as below: 

<br>

$$
h(\mathbf{x}) = \sum_{n=1}^{N} w_n \exp\left( -\gamma \|\mathbf{x} - \mathbf{x}_n\|^2 \right)
$$

<br>

Now we get an influence from every point in the data set. Now, let us describe why it is called raidal, basis funtion. It is radial because of this: 

<br>

$$
\underbrace{\|\mathbf{x} - \mathbf{x}_n\|}_{\text{radial}}
$$

<br>

And it is basis function because of this: 

<br>

$$
\underbrace{\exp\left( -\gamma \|\mathbf{x} - \mathbf{x}_n\|^2 \right)}_{\text{basis function}}
$$

<br>

This is our building block. We could used another basis function. So we could have another shape, that is also symmetric in center, and has the influence in a different way. And we will see an example later on. But this is basically the model in its simplest form, and its most popular form. Now we have the model, so, let us move on to the learning algorithm. 

Our goal is to find the optimal weights $w_n$ based on $N$ training examples: 

<br>

$$
(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_N, y_N)
$$

<br>

Since we have $N$ equaitions of $h(\mathbf{x}_n) = y_n$, we can find every $N$ weights. So, our goal is to mimize in-sample-error. After finding optimal weights, $h(\mathbf{x}_n} = y_n$ can be expressed as below: 

<br>

$$
\sum_{m=1}^{N} w_m \exp\left( -\gamma \|\mathbf{x}_n - \mathbf{x}_m\|^2 \right) = y_n
$$

<br>

We want this to be true for every $n$ from $1$ to $N$. Let us go for the solution. The basis function of equation above can be desribed using matrix notation as below: 

<br>

$$
\underbrace{
\begin{bmatrix}
\exp\left(-\gamma \|\mathbf{x}_1 - \mathbf{x}_1\|^2\right) & \cdots & \exp\left(-\gamma \|\mathbf{x}_1 - \mathbf{x}_N\|^2\right) \\
\exp\left(-\gamma \|\mathbf{x}_2 - \mathbf{x}_1\|^2\right) & \cdots & \exp\left(-\gamma \|\mathbf{x}_2 - \mathbf{x}_N\|^2\right) \\
\vdots & \ddots & \vdots \\
\exp\left(-\gamma \|\mathbf{x}_N - \mathbf{x}_1\|^2\right) & \cdots & \exp\left(-\gamma \|\mathbf{x}_N - \mathbf{x}_N\|^2\right)
\end{bmatrix}
}_{\boldsymbol{\Phi}}
$$

<br>

Then we multiply this by weight vector matrix: 

<br>

$$
\underbrace{
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_N
\end{bmatrix}
}_{\mathbf{w}}
$$

<br>

And we want this quantity to be equal with the following vector: 

<br>

$$
\underbrace{
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{bmatrix}
}_{\mathbf{y}}
$$

<br>

Now, if $\Phi$ is invertible, we can simply compute $\mathbf{w}$ exactly as below: 

<br>

$$
\mathbf{w} = \boldsymbol{\Phi}^{-1} \mathbf{y}
$$

<br>

Now let us look at the effect of $\gamma$. Consider we are given three data points. What happens when $\gamma$ is small? Our situation is illustrated in the following figure: 

![solution](/assets/images/rbf_2.svg) 

The total contribution of the three interpolations passes exactly through the points, because this is what we solved for. We can get a succesful interpolation. In contrast, if we go for a large $\gamma$, we will get: 

![solution](/assets/images/rbf_3.svg) 

The interpolation here is very poor, which clearly shows that $\gamma$ matters. The distance between data points also plays an important role, as we can see in the figures above. At the end of this post, we will discuss how to choose $\gamma$ wisely. 

For now, let us discuss RBF for classfication. Since $+1$ or $-1$ is also real-value, we can use RBF to classification. Our hypothesis will be formalized as below: 

<br>

$$
h(\mathbf{x}) = \mathrm{sign} \left( \sum_{n=1}^{N} w_n \exp\left( -\gamma \|\mathbf{x} - \mathbf{x}_n\|^2 \right) \right)
$$

<br>

And we are classifying data points depending on the signal 's': 

<br>

$$
\sum_{n=1}^{N} w_n \exp\left( -\gamma \|\mathbf{x} - \mathbf{x}_n\|^2 \right)
$$

<br>

As we discussed in [Lecture 2](https://isopink.github.io/Is-Learning-Feasible/), we want to minimize squared error: 

<br>

$$
\text{Minimize } (s - y)^2 
$$

<br>

--- 

#### 2. RBF with Nearest Neighbor

The Nearest Neighbor method classifies a new data point by copying the label of its closest neighbor in the training set. While this is simple and intuitive, it creates abrupt decision boundaries and is sensitive to noise. The situation is illustarted below: 

![solution](/assets/images/rbf_4.svg) 

To address the limitations of the NN method, we can introduce the KNN approach. Instead of relying on the single nearest point, KNN determines the label based on the distances to the K nearest neighbors.

RBF models build on this idea by considering the influence of many data points, not just the one point. Using smooth functions like the Gaussian, RBF assigns more weight to nearby points and less to distant ones, resulting in much smoother and more flexible boundaries. 

Now, let us discuss the KNN with RBF. Consider we are given $N$ data points with $N$ weight parameters. Instead of using all data points, we use only $K$ data points to be centered as $\mu$. Our hypothesis function will be described as below: 

<br>

$$
h(\mathbf{x}) = \sum_{k=1}^{K} w_k \exp\left( -\gamma \|\mathbf{x} - \boldsymbol{\mu}_k\|^2 \right)
$$

<br>

Here are two questions left. 

<br>

<div align="center">

  How to choose the centers $\boldsymbol{\mu}_k$?
  <br>
  How to choose the weights $w_k$?

</div>

<br>

