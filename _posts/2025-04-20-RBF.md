---
layout: single
title: "Lecture 16 : Radial Basis Functions"
---

In this time, we will discuss the RBF. Through this, we will learn how to solve problems using unsupervised learning for data without labels. The discussion will proceed in following four parts: 

1. RBF and nearest neighbors

2. RBF and neural networks

3. RBF and kernel methods

4. RBF and regularization

---

#### 1. Basic RBF model 

The basic idea of Radial Basis Function is that every point in the dataset affects the hypothesis. But, this is obvious since we build the hypothesis from the data. However, RBF receives influence in a special way through distance. In other words, one point in the dataset affects other nearby points more than the ones farther away. We can formalize it as below: 

<br>

<div align="center">
Each $(\mathbf{x}_n, y_n) \in \mathcal{D}$ influences $h(\mathbf{x})$ based on $\|\mathbf{x} - \mathbf{x}_n\|$
</div>

<br>

Now, let us llo at a figure below: 

![solution](/assets/images/rbf_1.svg) 

The center of this bump is $x_n$. This shows how the influence of $x_n$ on the neigoring points in the space. So, it is most influential nearby. Since it is affected by only distance, bump looks symmetric around. Now, let us introudce the standarad form of RBF model as below: 

<br>

$$
h(\mathbf{x}) = \sum_{n=1}^{N} w_n \exp\left( -\gamma \|\mathbf{x} - \mathbf{x}_n\|^2 \right)
$$

<br>

Now we get an influence from every point in the data set. Now, let us describe why it is called raidal, basis funtion. It is radial because of this: 

<br>

$$
\underbrace{\|\mathbf{x} - \mathbf{x}_n\|}_{\text{radial}}
$$

<br>

And it is basis function because of this: 

<br>

$$
\underbrace{\exp\left( -\gamma \|\mathbf{x} - \mathbf{x}_n\|^2 \right)}_{\text{basis function}}
$$

<br>

This is our building block. We could used another basis function. So we could have another shape, that is also symmetric in center, and has the influence in a different way. And we will see an example later on. But this is basically the model in its simplest form, and its most popular form. Now we have the model, so, let us move on to the learning algorithm. 

Our goal is to find the optimal weights $w_n$ based on $N$ training examples: 

<br>

$$
(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_N, y_N)
$$

<br>

Since we have $N$ equaitions of $h(\mathbf{x}_n) = y_n$, we can find every $N$ weights. So, our goal is to mimize in-sample-error. After finding optimal weights, $h(\mathbf{x}_n} = y_n$ can be expressed as below: 

<br>

$$
\sum_{m=1}^{N} w_m \exp\left( -\gamma \|\mathbf{x}_n - \mathbf{x}_m\|^2 \right) = y_n
$$

<br>

We want this to be true for every $n$ from $1$ to $N$. Let us go for the solution. The basis function of equation above can be desribed using matrix notation as below: 

<br>

$$
\underbrace{
\begin{bmatrix}
\exp\left(-\gamma \|\mathbf{x}_1 - \mathbf{x}_1\|^2\right) & \cdots & \exp\left(-\gamma \|\mathbf{x}_1 - \mathbf{x}_N\|^2\right) \\
\exp\left(-\gamma \|\mathbf{x}_2 - \mathbf{x}_1\|^2\right) & \cdots & \exp\left(-\gamma \|\mathbf{x}_2 - \mathbf{x}_N\|^2\right) \\
\vdots & \ddots & \vdots \\
\exp\left(-\gamma \|\mathbf{x}_N - \mathbf{x}_1\|^2\right) & \cdots & \exp\left(-\gamma \|\mathbf{x}_N - \mathbf{x}_N\|^2\right)
\end{bmatrix}
}_{\boldsymbol{\Phi}}
$$

<br>

Then we multiply this by weight vector matrix: 

<br>

$$
\underbrace{
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_N
\end{bmatrix}
}_{\mathbf{w}}
$$

<br>

And we want this quantity to be equal with the following vector: 

<br>

$$
\underbrace{
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{bmatrix}
}_{\mathbf{y}}
$$

<br>

Now, if $\Phi$ is invertible, we can simply compute $\mathbf{w}$ exactly as below: 

<br>

$$
\mathbf{w} = \boldsymbol{\Phi}^{-1} \mathbf{y}
$$

<br>

Now let us look at the effect of $\gamma$. Consider we are given three data points. What happens when $\gamma$ is small? Our situation is illustrated in the following figure: 

![solution](/assets/images/rbf_2.svg) 

The total contribution of the three interpolations passes exactly through the points, because this is what we solved for. We can get a succesful interpolation. In contrast, if we go for a large $\gamma$, we will get: 

![solution](/assets/images/rbf_3.svg) 

The interpolation here is very poor, which clearly shows that $\gamma$ matters. The distance between data points also plays an important role, as we can see in the figures above. At the end of this post, we will discuss how to choose $\gamma$ wisely. 

For now, let us discuss RBF for classfication. Since $+1$ or $-1$ is also real-value, we can use RBF to classification. Our hypothesis will be formalized as below: 

<br>

$$
h(\mathbf{x}) = \mathrm{sign} \left( \sum_{n=1}^{N} w_n \exp\left( -\gamma \|\mathbf{x} - \mathbf{x}_n\|^2 \right) \right)
$$

<br>

And we are classifying data points depending on the signal 's': 

<br>

$$
\sum_{n=1}^{N} w_n \exp\left( -\gamma \|\mathbf{x} - \mathbf{x}_n\|^2 \right)
$$

<br>

As we discussed in [Lecture 2](https://isopink.github.io/Is-Learning-Feasible/), we want to minimize squared error: 

<br>

$$
\text{Minimize } (s - y)^2 
$$

<br>

--- 

#### 2. RBF with Nearest Neighbor

The Nearest Neighbor method classifies a new data point by copying the label of its closest neighbor in the training set. While this is simple and intuitive, it creates abrupt decision boundaries and is sensitive to noise. The situation is illustarted below: 

![solution](/assets/images/rbf_4.svg) 

To address the limitations of the NN method, we introduce the KNN approach. Instead of relying on the single nearest point, KNN determines the label based on the distances to the $K$ nearest neighbors.

RBF models build on this idea by considering the influence of many data points, not just the one point. Using smooth functions like the Gaussian, RBF assigns more weight to nearby points and less to distant ones, resulting in much smoother and more flexible boundaries. 

Now, let us discuss KNN with RBF. Suppose we are given $N$ data points and $N$ corresponding weight parameters. Instead of using all data points as centers, we select only $K$ representative points as centers, denoted by $\boldsymbol{\mu}_k$. Our hypothesis function is then given as follows:

<br>

$$
h(\mathbf{x}) = \sum_{k=1}^{K} w_k \exp\left( -\gamma \|\mathbf{x} - \boldsymbol{\mu}_k\|^2 \right)
$$

<br>

Here are two questions left. 

<br>

<div align="center">

  How to choose the centers $\boldsymbol{\mu}_k$?
  <br>
  How to choose the weights $w_k$?

</div>

<br>

##### 2.1. How to choose $\mu$? 

Let us first discuss how to choose the centers. One way to deal with this is to choose centers that minimize the distance between each data point $\mathbf{x}_n$ and its closest center $\mu_k$. This method is known as K-means clustering, as the centers eventually become the mean of the data points in each cluster. So, we our task will focus on the following:

<br>

$$
\text{Split } \mathbf{x}_1, \cdots, \mathbf{x}_N \text{ into clusters } S_1, \cdots, S_K
$$

<br>

These clusters should be formed in a way that: 

<br>

$$
\text{Minimize } \sum_{k=1}^{K} \sum_{\mathbf{x}_n \in S_k} \|\mathbf{x}_n - \boldsymbol{\mu}_k\|^2
$$

<br>

The advantage of this method is that it is unsupervised learning, so it can be applied without depending on $y_n$. However, a major drawback is that the method is NP-hard, which means its time complexity cannot be expressed as a polynomial function. So, we should apply heuristic or iterative method. The most famous method is Lloyd's algorithm. 

Let us now introduce this algorithm. Our goal is to minimize the objective function with respect to both $\mu_k$ and $S_k$. The algorithm proceeds by fixing one and optimizing the other. Given a particular clustering of the data, we compute the optimal centers that minimize the total within-cluster distance as below: 

<br>

$$
\boldsymbol{\mu}_k \leftarrow \frac{1}{|S_k|} \sum_{\mathbf{x}_n \in S_k} \mathbf{x}_n
$$

<br>

Then, with the updated centers, each data point is reassigned to the closest center, effectively updating the clusters as below: 

<br>

$$
S_k \leftarrow \left\{ \mathbf{x}_n : \|\mathbf{x}_n - \boldsymbol{\mu}_k\| \leq \text{all } \|\mathbf{x}_n - \boldsymbol{\mu}_\ell\| \right\}
$$

<br>

This process is repeated until the assignments no longer change. It will converge to local minimum or another. Let us look at the actual example, same as we used in [Lecture 14](https://isopink.github.io/SVM/). The data points are labled with $+1$ and $-1$, and they are slightly non-separable. 

![solution](/assets/images/rbf_5.svg) 

Then we randomly choose the centers as below: 

![solution](/assets/images/rbf_6.svg) 

Although deciding how many centers to use is an important issue, in this case, we set the number to $9$ for the purpose of comparison with the SVM example. We then iterate the Lloyd's algorithm until the cluster assignments no longer change. Here is the result: 

![solution](/assets/images/rbf_7.svg) 

These points are our $\mu_k$'s. Now, let us examine the resulting decision boundary, even though the algorithm operates without access to the actual labels:

![solution](/assets/images/rbf_8.svg) 

Using this method, the classification result looks like the plot above. One poor classification occurs at the bottom-left center, whose cluster contains both $+1$ and $-1$ labels. As a result, the RBF fails there, but this is a trade-off we accept when using unsupervised learning.
Now, let us compare Support vectors versus RBF centers. 

![solution](/assets/images/rbf_9.svg) 

Support vectors are representative points of the dataset, and so are the centers in an RBF model. Although both serve as representative points, they differ entirely in how they are obtained and what roles they play. 

Support vectors are obtained through supervised learning, whereas RBF centers are determined through unsupervised learning. While support vectors are always chosen from the training data, RBF centers may or may not correspond to actual data points. In terms of their roles, support vectors contribute directly to defining the separating surface between classes, whereas RBF centers serve to represent the underlying input distribution. Additionally, support vectors are explicitly associated with class labels ($+1$ or $-1$), but RBF centers carry no such label information.

##### 2.2. How to choose $w$? 

As we finishied choosing $\mu_k$, let us discuss the second question. We want our hypothesis to be as below: 

<br>

$$
\sum_{k=1}^{K} w_k \exp\left( -\gamma \|\mathbf{x}_n - \boldsymbol{\mu}_k\|^2 \right) \approx y_n
$$

<br>

And we can describe the basis function as $N$ by $K$ matrix as below: 

<br>

$$
\underbrace{
\begin{bmatrix}
\exp\left( -\gamma \|\mathbf{x}_1 - \boldsymbol{\mu}_1\|^2 \right) & \cdots & \exp\left( -\gamma \|\mathbf{x}_1 - \boldsymbol{\mu}_K\|^2 \right) \\
\exp\left( -\gamma \|\mathbf{x}_2 - \boldsymbol{\mu}_1\|^2 \right) & \cdots & \exp\left( -\gamma \|\mathbf{x}_2 - \boldsymbol{\mu}_K\|^2 \right) \\
\vdots & \ddots & \vdots \\
\exp\left( -\gamma \|\mathbf{x}_N - \boldsymbol{\mu}_1\|^2 \right) & \cdots & \exp\left( -\gamma \|\mathbf{x}_N - \boldsymbol{\mu}_K\|^2 \right)
\end{bmatrix}
}_{\boldsymbol{\Phi}}
$$

<br>

Then we multiply this by weight vector matrix $mathbf{w}$: 

<br>

$$
\underbrace{
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_K
\end{bmatrix}
}_{\mathbf{w}}
$$

<br>

Finally, we want this quantity to approximate lable $y$: 

<br>

$$
\underbrace{
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{bmatrix}
}_{\mathbf{y}}
$$

<br>

However, since $\Phi$ is not square matrix, there is no direct inverse matrix. Instead, we use should apply pesudo-inverse, discussed in [Lecture 3](https://isopink.github.io/Linear-Model-L/) Then, weight vector matrix $mathbf{w}$ can be computed as below: 

<br>

$$
\mathbf{w} = \left( \boldsymbol{\Phi}^\top \boldsymbol{\Phi} \right)^{-1} \boldsymbol{\Phi}^\top \mathbf{y}
$$

<br>

In this case, we are not guaranteed that we will get the correct value at every data point. So we are going to make an in-sample error. But we know that this is not a bad thing. On the other hand, we are only determining $K$ weights. So the chances of generalization are good. Now, we would like to take this, and put it as a graphical network. 

--- 

#### 3. RBF with graphical network 

We already related RBF to nearest neighbor methods, similarity methods. Now we are going to relate it to neural networks. Let us first put the diagram. Here is illustration of it: 

![solution](/assets/images/rbf_10.svg) 

We have $x$. We are computing the radial aspect, the distance from $\mu_1$ up to $\mu_K$, and then handing it to a nonlinearity, in this case the gaussian nonlinearity. We can have other basis functions. These features are then linearly combined using weights $w_k$ to generate the final output.

This setup closely mirrors a neural network with one hidden layer. The first layer extracts nonlinear features based on the distance to fixed centers, and the second layer performs a linear combination of those features to produce the output.

A key distinction in RBF models is that the centers $\mu_k$ are pre-determined, often using algorithms like K-means, and remain fixed throughout training. As a result, only the weights $w_k$ need to be learned, and since the model is linear in $w$, they can be efficiently computed using the pseudo-inverse. In some cases, a bias term such as $w_0$ or $b$ is added to the output layer, which is equivalent to including a constant input of $1$.

However, in addition to the weights $w_k$, we also have the parameter $\gamma$, which controls the width of the Gaussian functions. Choosing a good value for $\gamma$ is crucialâ€”if it's too small or too large, the interpolation quality may degrade significantly. To address this, we can apply an alternating optimization strategy, similar to the Expectation-Maximization (EM) algorithm.

We first fix $\gamma$ and compute the optimal weights $w_k$ using the pseudo-inverse. Then, keeping the weights fixed, we update $\gamma$ using gradient descent to minimize the in-sample error. This process repeats iteratively until convergence.

Moreover, instead of using a single shared $\gamma$, we can allow each center to have its own width $\gamma_k$. While this increases the number of parameters, the model remains tractable and gains flexibility, enabling it to better adapt to varying local densities in the input space.
