---
layout : single
title : Maximum Likelihood Estimation
--- 

In this post, we are goind to discuss MLE. We will explore the intution, math, and application of MLE from simple examples to its connection with linear regression and cross-entropy.


---

###  Goal of MLE

Suppose we observe data points:

$$
\mathcal{D} = \{x^{(1)}, x^{(2)}, \dots, x^{(m)}\}
$$

We assume that these data points are independently sampled from an unknown distribution \( \mathrm{pdata}(x) \). We approximate this with a parametric model \( \mathrm{pmodel}(x;\theta) \). Our goal is to find the parameter \( \theta \) that maximizes the likelihood of observing our dataset:

$$
\theta_{\mathrm{ML}} = \arg\max_{\theta} \prod_{i=1}^{m} \mathrm{pmodel}(x^{(i)};\theta)
$$

Taking the log simplifies this:

$$
\theta_{\mathrm{ML}} = \arg\max_{\theta} \sum_{i=1}^{m} \log \mathrm{pmodel}(x^{(i)};\theta)
$$

This is the **log-likelihood**, and its maximization is at the heart of MLE.

---

### üîç MLE as Minimizing KL Divergence

We can also interpret MLE as minimizing the **Kullback-Leibler divergence** between the empirical distribution \( \hat{p}_{\mathrm{data}} \) (formed by the dataset) and the model distribution:

$$
D_{\mathrm{KL}}(\hat{p}_{\mathrm{data}} \,\|\, \mathrm{pmodel}) = \mathbb{E}_{x \sim \hat{p}_{\mathrm{data}}} \left[ \log \hat{p}_{\mathrm{data}}(x) - \log \mathrm{pmodel}(x) \right]
$$

Since \( \log \hat{p}_{\mathrm{data}}(x) \) doesn't depend on \( \theta \), minimizing the KL divergence is equivalent to:

$$
\theta_{\mathrm{ML}} = \arg\min_{\theta} \left( - \mathbb{E}_{x \sim \hat{p}_{\mathrm{data}}} [\log \mathrm{pmodel}(x;\theta)] \right)
$$

This is the **cross-entropy** between the data and the model.

---

###  Example: Linear Regression via MLE

In linear regression, we assume:

$$
y = \mathbf{w}^\top \mathbf{x} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
$$

This gives the conditional probability:

$$
\mathrm{p}(y \mid \mathbf{x}; \mathbf{w}) = \mathcal{N}(y \mid \mathbf{w}^\top \mathbf{x}, \sigma^2)
$$

Maximizing the log-likelihood becomes:

$$
\sum_{i=1}^{m} \log \mathcal{N}(y^{(i)} \mid \mathbf{w}^\top \mathbf{x}^{(i)}, \sigma^2)
$$

Which simplifies to minimizing:

$$
\sum_{i=1}^{m} \left( y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} \right)^2
$$

This is just **mean squared error (MSE)**! So, **linear regression is MLE under Gaussian noise**.

---

###  Properties of MLE

MLE has several important theoretical properties:

- **Consistency**: As the number of data points \( m \to \infty \), the MLE converges to the true parameter \( \theta \) (assuming the model is correct).
- **Asymptotic efficiency**: Among consistent estimators, MLE achieves the lowest possible variance (Cram√©r-Rao lower bound).
- **Invariance**: Functions of MLEs are MLEs of the functions.

However, MLE can **overfit** when data is limited. That‚Äôs where regularization and Bayesian methods come in.

---

###  MAP Estimation vs. MLE

**Maximum A Posteriori (MAP)** estimation incorporates prior knowledge:

$$
\theta_{\mathrm{MAP}} = \arg\max_{\theta} \log \mathrm{p}(x \mid \theta) + \log \mathrm{p}(\theta)
$$

This is just **MLE plus a regularization term** from the prior! For example, using a Gaussian prior \( \mathrm{p}(\theta) \sim \mathcal{N}(0, \lambda^{-1} I) \) gives:

$$
\theta_{\mathrm{MAP}} = \arg\min_{\theta} \left[ - \log \mathrm{p}(x \mid \theta) + \lambda \|\theta\|^2 \right]
$$

So, **weight decay in neural networks is equivalent to MAP with a Gaussian prior**.

---

###  Summary

- MLE chooses parameters that maximize the likelihood of observed data.
- Equivalent to minimizing the KL divergence between model and empirical distributions.
- MLE underlies many standard loss functions (e.g., MSE, cross-entropy).
- Has strong theoretical guarantees: consistency and efficiency.
- Bayesian and MAP approaches extend MLE by incorporating prior beliefs.

---


