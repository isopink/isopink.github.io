---
layout: single
title: "Lecture 4 : Error and Noise"
---

In this time, we will discuss about error and noise  it will be used for deeper concepts later. The discussion will proceed in the following three parts :

1. Error measures

2. Noisy targets

---

#### 1. Error measures 

When can we say that our hypothesis approximates the target funtion well? In other words, what does $h \approx f$ means? 
we need to define an error measure that qunatifies how far we are from the target funtion. Let's formalize the notion first: 

<br>

$$
\text{Error} = E(h, f).
$$

<br>

 It is almost always defined based on pointwise. - the individual input points $x$. if we define point wise error measure $e(h(x),f(x))$, the overall error will be the average value of pointwise errors. We now update the notation. 

<br>

$$
E_{\text{in}}(h) = \frac{1}{N} \sum_{n=1}^{N} e\left(h(\mathbf{x}_n), f(\mathbf{x}_n)\right)
$$

<br>

$$
E_{\text{out}}(h) = \mathbb{E}_{\mathbf{x}} \left[ e\left(h(\mathbf{x}), f(\mathbf{x})\right) \right]
$$

<br>

We already discussed the *in - sample error* in [__previous time__](https://isopink.github.io/Is-Learning-Feasible/). You may ask how to calculate *the out-of-sample error*. We will discuss it later. For now on, we just keep going. **The important thing is that the error measure can vary depending on the context**.  Consider the problem of verifying fingerpint belongs to a particular person. There are two types of error that our hypothesis make here. 

![solution](/assets/images/err_1.svg) 

If the correct person is rejected, ( $h = -1$ but $f = +1$), it is called *false reject*, and if an incorrect person is accepted ($h = +1$ but $f = -1$), it is called *false accept*. How should the error measure be defined in this problem? We need to specify the error values for a false accept and for a false reject. The right values depend on the situation. 


##### 1.1. Supermarket case


For the supermarket, **a false reject is costly** because if a customer gets wrongly rejected, the customer may be annoyed. All future revenue from this annoyed customer is lost. On the other hand, the cost of a false accept is minor. You just gave away a discount to someone who didn’t deserve it, and that person left their fingerprint in your system – they must be bold indeed. The matrix would like: 

<br>

$$
\begin{array}{c|cc}
h \backslash f & +1 & -1 \\
\hline
+1 & 0 & 1 \\
-1 & 10 & 0 \\
\end{array}
$$

<br>

##### 1.2. CIA case

For the CIA, **a false accept is a disaster**. An unauthorized person will gain access to a highly sensitive facility. This should be reflected in a much higher cost for the false accept. False rejects, on the other hand, can be tolerated since authorized persons are employees (rather than customers as with the supermarket). The inconvenience of retrying when rejected is just part of the job, and they must deal with it. The matrix would like: 

<br>

$$
\begin{array}{c|cc}
h \backslash f & +1 & -1 \\
\hline
+1 & 0 & 1000 \\
-1 & 1 & 0 \\
\end{array}
$$

<br>

---

#### 2. Noisy Targets  

So far, we are trying to learn target 'function', indeed, **our target may not be 'function'**. Consider credit approval example [__before__](https://isopink.github.io/Learning-problem/), two customers may have identical applicants, but end up with diffrerent result. The output $y$ could no be uniquely determined by $x$. Actually, we can get through this issue by $y$ is afftected by $x$. Therefore, we use target 'distribution', not the target 'funtion'. Instead of $y=f(x)$, we can take the output $y$ to be a random variable, that is affected by, rather than determined by the input $x$. We can formalize this : 

<br>

$$
P(y \mid \mathbf{x})
$$

<br>

A data point $(x,y)$ is now generated by the joint distribution: 

<br>

$$
P(\mathbf{x}) \, P(y \mid \mathbf{x})
$$

<br>

Based on these ideas, we now define the **Noisy target**. We can think of it as a deterministic target plus noise. If $y$ is a *real-valued*, we can take the expected value of $y$ given $x$ to be the deterministic $f(x)$ — $f(\mathbf{x}) = \mathbb{E}(y \mid \mathbf{x})$ — and $y- f(x)$ as pure noise. So we can interpret output $y$ as: 

<br>

$$
y = f(\mathbf{x}) + \underbrace{\left(y - f(\mathbf{x})\right)}_{\text{noise}}
$$

<br>

This view suggests that a **deterministic target function can be considered a special case of a noisy target**, just with zero noise. Indeed, we can formally express any function $f$ as a distribution $P(y \mid \mathbf{x})$ by choosing $P(y \mid \mathbf{x})$ to be zero for all $y$ except $y = f(\mathbf{x})$. Here is the final learning diagram: 

![solution](/assets/images/err_2.svg) 

Both $P(y \mid \mathbf{x})$ and $P(\mathbf{x})$ describe probabilistic aspects of the learning problem, but they play very different roles. The Target Distribution $P(y \mid \mathbf{x})$ is what we are trying to learn. It explains why we may get different $y$ values even for the same $\mathbf{x}$. It reflects the uncertainty or noise in the output. The Input Distribution $P(\mathbf{x})$ tells us how frequently each input $\mathbf{x}$ appears. It indirectly determines which inputs are more important during learning.


In practice, we get data like ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_N, y_N)}$ drawn from the joint distribution $P(\mathbf{x}, y)$. This joint distribution mixes two sources of uncertainty —  input frequency and output noise. But our actual goal is to learn only $P(y \mid \mathbf{x})$, so we need to keep both clearly separated.



