---
layout: single
title: "Lecture 14 : Support Vector Machine"
---

In this time, we are going to discuss the Support Vector Machine. The discussion will proceed in the following three parts: 

1. Maximizing the margin

2. The solution

3. Nonlinear transforms

--- 

#### 1. Maximizing the margin 

![solution](/assets/images/svm_1.svg) 

Suppose we are given linearly separable data as above figure illustrates. Which line could be the best boundary line? We can draw a line as below: 

![solution](/assets/images/svm_2.svg) 

Is it the best line? When is it going to start making an error? This is the concept of margin, highlited in yellow. Let us draw another line:

![solution](/assets/images/svm_3.svg) 

It seems better than the previous line, since it has wider margin. Let us try to get the best margin:

![solution](/assets/images/svm_4.svg) 

We finally get the widest margin. Can we say that this one is the best boundary line? $E_{\text{in}}$ are all zero, and the generalization performance will be same since we have four points equally. Nonetheless, we may agree with the third one is the best choice, intuitively. Let us now consider two quenstions. 

<br>

<div align="center">

Why is bigger margin better?

<br>

<br>

Which $\mathbf{w}$ maximizes the margin?

</div>

<br>

##### 1.1. Why is bigger margin better? 

Consider we are allowed to use a classifeir, but we have to have at least some margin to accept it. The situation is illustrated in the following figure. 

![solution](/assets/images/svm_5.svg) 

If the least margin should be greater than case (b), we are putting a restriction on the [growth function](https://isopink.github.io/Effective-number-of-hypothesis/). Fat margins imply fewer [dichotomies](https://isopink.github.io/Effective-number-of-hypothesis/) possible. Therefore, we have smaller [VC dimension](https://isopink.github.io/VC-Dimension/), than if we didn't restrict them. 

Although this is informal, we will come to a result that estimates the $E_{\text{out}}$ based on the margin, at the end of this post. And we will find out that indeed, bigger margin enables better $E_{\text{out}}$. So, we now try to believe that the fat margins are good. Let us solve for them. 

<br>

##### 1.2. Which $\mathbf{w}$ maximizes the margin?

Let $x_n$ be the nearest data point to the plane (or hyperplane) $\mathbf{w}^\top \mathbf{x} = 0$. The distance from $x_n$ to the plane will be the margin. Now, there are two prelimaries that we are going to invoke here. They will simplify the analysis later on. First, normalize $\mathbf{w}$ as below: 

<br>

$$
\left| \mathbf{w}^\top \mathbf{x}_n \right| = 1
$$

<br>

Even if we scale $\mathbf{w}$, the plane remains unchanged and the margin is scale invariace. In order to simplify the analysis later, we can normalize $\mathbf{w}$, without loss of generality. Second, pull out $w_0$: 

<br>

$$
\mathbf{w} = (w_1, \cdots, w_d)
$$

<br>

So far, we are adding $w_0$ for the nice representation of vector and matrix. However, when we solove for the margin, it is no longer convenient. So, for the analysis of support vector machines, we are going to pull $w_0$ out. And in order not to confuse it, we are going to call $w_0$ here $b$, for bias. Then the equation for the plane becomes: 

<br>

$$
\mathbf{w}^\top \mathbf{x} + b = 0
$$

<br>

Remember, there is no $x_0$ in the vector $\mathbf{x}$ here. We will keep using this convention that will make our math more friendly. Again, we want to compute the distance from $x_n$ to the plane. Let us look at the geometry of the situation. 

![solution](/assets/images/svm_6.svg) 

On the figure above, the vector $\mathbf{w}$ is perpendicular to the plane in the input space $\mathcal{X}$. Let $\mathbf{x}'$ and $\mathbf{x}''$ is on the plane. Then, 

<br>

$$
\mathbf{w}^\top \mathbf{x}' + b = 0 \quad \text{and} \quad \mathbf{w}^\top \mathbf{x}'' + b = 0
$$

<br>

If we take the difference between these two equations: 

<br>

$$
\mathbf{w}^\top (\mathbf{x}' - \mathbf{x}'') = 0
$$

<br>

Since we did not make any restrictions on $\mathbf{x}'$ and $\mathbf{x}''$, vector $\mathbf{w}$ must be orthogonal to every vector on the plane. Now we can get the distance. 

![solution](/assets/images/svm_7.svg) 

First, pick any point on the plane. We just call it generic $\mathbf{x}$. Then, project the vector from $x$ to $x_n$ on $\mathbf{w}$. The distance can be described as:

<br>

$$
\left| \hat{\mathbf{w}}^\top (\mathbf{x}_n - \mathbf{x}) \right|, \quad
\text{where } \hat{\mathbf{w}} = \frac{\mathbf{w}}{\|\mathbf{w}\|}
$$

<br>

This can be simplifeid if we add the missing term bias as: 

<br>

$$
\frac{1}{\|\mathbf{w}\|} \left| \mathbf{w}^\top \mathbf{x}_n + b - \mathbf{w}^\top \mathbf{x} - b \right| = \frac{1}{\|\mathbf{w}\|}
$$

<br>

where $\mathbf{w}^\top \mathbf{x} + b = 0$ as we mentioned before. Let us now formulate the problem. We want to compute: 

<br>

$$
\max \frac{1}{\|\mathbf{w}\|} \quad \text{subject to } \min_{n = 1, \dots, N} \left| \mathbf{w}^\top \mathbf{x}_n + b \right| = 1
$$

<br>

In fact, minimum is not a nice function to have. So we are going to try to find an equivalent problem that is more friendly. Since Every point is supposed to be classified correctly, notice: 

<br>

$$
\left| \mathbf{w}^\top \mathbf{x}_n + b \right| = y_n \left( \mathbf{w}^\top \mathbf{x}_n + b \right)
$$

<br>

Because they are classifying the points correctly, the signal have to agree with the label, $+1$ or $-1$. Therfore,  We can use this term instead of the absolute value. Next, instead of maximimizing $1$ over the norm, we are going to minimize norm. Then our whole formula becomes: 

<br>

$$
\min \frac{1}{2} \mathbf{w}^\top \mathbf{w} \quad \text{subject to } y_n(\mathbf{w}^\top \mathbf{x}_n + b) \geq 1
$$

<br>

We multiply $\frac{1}{2}$ for the computational convenience. At least one one of the points will satisfies with equality, which means that the minimum is $1$. Even if there are no such points, we just modify $\mathbf{w}$ and $b$, to make it $1$. Therefore this problem is equivalent to the previous problem. Now let us look at the solution. 

---

#### 2. The solution 

Now we have a constrained optimization. Geometry won't help us very much, so we need take an analytical approach to solve it. We are going to use the Lagrangian method on the KKT (Karush-Kuhn-Tucker) conditions. First, take the inequality constraints and put them in the $0$ form as: 

<br>

$$
y_n (\mathbf{w}^\top \mathbf{x}_n + b) - 1 
$$

<br>

Instead of equal or greather than $1$, the original constraints, it is equal or greater than $0$. Then, muliply by Lagrange muliplier $\alpha \geq 0$, and add them up: 

<br>

$$
\sum_{n=1}^{N} \alpha_n \left( y_n (\mathbf{w}^\top \mathbf{x}_n + b) - 1 \right)
$$

<br>

Finally, let them become the part of the objective function as below: 

<br>

$$
\frac{1}{2} \mathbf{w}^\top \mathbf{w} - \sum_{n=1}^{N} \alpha_n \left( y_n (\mathbf{w}^\top \mathbf{x}_n + b) - 1 \right)
$$

<br>

The formula above is denoted by $\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})$. And we want to minimize with respect to $\mathbf{w}$ and $b$. 

The interesting part we should pay attention to is that we are actually maximizing with respect to $\alpha$. This is interesting because when we had equality, we don't need to worry about maximization versus minimization, since we can get the graident equals to $0$. However, we are maximizing with respect to $\alpha$, the inequality. It may be impossible to get the gradient to be $0$. So the question of maximizing versus minimizing, we need to pay attention here. Fortunately, we will just tell the Quadratic programming guy (QP solver), please maximize it. 

Now we do at least the unconstrained part. We are going to take the gradient of the Lagrangian, With respect to $\mathbf{w}$ and $b$ as below: 

<br>

$$
\nabla_{\mathbf{w}} \mathcal{L} = \mathbf{w} - \sum_{n=1}^{N} \alpha_n y_n \mathbf{x}_n = 0
$$

<br>

$$
\frac{\partial \mathcal{L}}{\partial b} = - \sum_{n=1}^{N} \alpha_n y_n = 0
$$

<br>

These are the conditions we have to satisfy. Now, we will substitute these conditions into the original Lagrangian so that the maximization over $\alpha$—which has constraints—no longer depends on $\mathbf{w}$ and $b$. This is known as the dual formulation of the problem. Let us substitute now. We have two equations. 

<br>

$$
\mathbf{w} = \sum_{n=1}^{N} \alpha_n y_n \mathbf{x}_n \quad \text{and} \quad \sum_{n=1}^{N} \alpha_n y_n = 0
$$

<br>

We want to substitute $\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})$. Then, we get: 

<br>

$$
\mathcal{L}(\boldsymbol{\alpha}) = \sum_{n=1}^{N} \alpha_n - \frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_n y_m \alpha_n \alpha_m \, \mathbf{x}_n^\top \mathbf{x}_m
$$

<br>

Now, let us look at the maximization with respect to $\alpha$ subject to $\alpha_n \geq 0$, for $n = 1, \cdots, N$ and $\sum_{n=1}^{N} \alpha_n y_n = 0$. However, quadratic programming packages come usually with minimization. So we need to translate our optimization into minimization. The formula becomes: 

<br>

$$
\min_{\boldsymbol{\alpha}} \ \frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_n y_m \alpha_n \alpha_m \, \mathbf{x}_n^\top \mathbf{x}_m - \sum_{n=1}^{N} \alpha_n
$$

<br>

We can describe this using matrix notation as: 

<br>

$$
\min_{\boldsymbol{\alpha}} \ \frac{1}{2} \boldsymbol{\alpha}^\top \mathbf{Q} \boldsymbol{\alpha} - \mathbf{1}^\top \boldsymbol{\alpha} 
\quad \text{subject to } \ \mathbf{y}^\top \boldsymbol{\alpha} = 0, \quad \boldsymbol{\alpha} \geq 0
$$

<br>

$\mathbf{Q}$ is defined as an $N \times N$ matrix where each element is given by $Q_{nm} = y_n y_m \mathbf{x}_n^\top \mathbf{x}_m$. If we give $Q$ to the QP solver, with constraints conditions, it will give us back an $\alpha$: 

<br>

$$
\text{Solution:} \quad \boldsymbol{\alpha} = \alpha_1, \cdots, \alpha_N
$$

<br>

Then, we can get the $\mathbf{w}$ as below: 

<br>

$$
\mathbf{w} = \sum_{n=1}^{N} \alpha_n y_n \mathbf{x}_n
$$

<br>

We now introduce one more KKT condition, Complementary Slackness as below: 

<br>

$$
\text{For } n = 1, \cdots, N, \quad \alpha_n \left( y_n \left( \mathbf{w}^\top \mathbf{x}_n + b \right) - 1 \right) = 0
$$

<br>

If a data point lies outside the margin, then the slack term, defined as $y_n \left( \mathbf{w}^\top \mathbf{x}_n + b \right) - 1$, is positive. In this case, $\alpha_n = 0$. Conversely, if $\alpha_n > 0$, the slack is zero, meaning that the data point lies exactly on the margin boundary. We call this data point Support Vector. In summary: 

<br>

$$
\alpha_n > 0 \ \Longrightarrow \ \mathbf{x}_n \text{ is a support vector}
$$

<br>

As we already computed $\mathbf{w}$, we are now able to compute $b$, using any support vector. We just solve: 

<br>

$$
y_n \left( \mathbf{w}^\top \mathbf{x}_n + b \right) = 1
$$

<br>

---

#### 3. Nonlinear transforms 

We would like to see what happens to the problem of support vector machines, when we move to the higher dimensional space. Is the problem becoming more difficult? Does it hold? So let us look at it. Consider input space $\mathcal{X}$ is given as below: 

![solution](/assets/images/svm_8.svg) 

This data is not linearly separable. We need a [nonlinear transformation](https://isopink.github.io/Linear-Model-L/). Let us take $\mathbf{x}_1$ squared and $\mathbf{x}_2$ squared. Then we get this: 

![solution](/assets/images/svm_9.svg) 

What we are doing now is working in the $\mathcal{Z}$ space. That is to say, we need $\mathbf{z}$ instead of $\mathbf{x}$. So our Lagrangian formual will be: 

<br>

$$
\mathcal{L}(\boldsymbol{\alpha}) = \sum_{n=1}^{N} \alpha_n - \frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_n y_m \, \alpha_n \alpha_m \, \mathbf{z}_n^\top \mathbf{z}_m
$$

<br>

This raises a question. What if the dimensionality of the $Z$ space is too high? There's no need to worry. What we get is simply a longer $\mathbf{z}$ vector, and we still only deal with inner product. Moreover, we still pass an $N \times N$ coefficient matrix to the QP solver, just like before. In this case, support vector and the margin is maintained in $\mathcal{Z}$ space. In $\mathcal{X}$ space, we can observe the pre-images of support vector. Let us look at the another input sapce: 

![solution](/assets/images/svm_10.svg) 

Suppose the dimensionality of $\mathcal{Z}$ is a million. Highlighted four points are pre-images of support vectors. Here is the remarkable part: Even in a million-dimensional space, the decision boundary can be defined using just four support vectors. This means that in the $\mathcal{Z}$ space, $\mathbf{w}$ is defined using just 4 parameters. Therefore, support vectors have properties that make them highly suitable for generalization. Let us look at the generalization result: 

<br>

$$
\mathbb{E}[E_{\text{out}}] \leq \mathbb{E}\left[ \frac{\text{number of support vectors}}{N - 1} \right]
$$

<br>

We do not have to pay for the computation of going to the higher dimension, and generalization. This is why nonlinear SVM method are powerful. Even better, when we apply kernel methods, we avoid even the cost of computing inner products directly. We will explore this more in the next lecture. 






