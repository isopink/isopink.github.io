---
layout: single
title: "Lecture 14 : Support Vector Machine"
---

In this time, we are going to discuss the Support Vector Machine. The discussion will proceed in the following three parts: 

1. Maximizing the margin

2. The solution

3. Nonlinear transforms

--- 

#### 1. Maximizing the margin 

![solution](/assets/images/svm_1.svg) 

Suppose we are given linearly separable data as above figure illustrates. Which line could be the best boundary line? We can draw a line as below: 

![solution](/assets/images/svm_2.svg) 

Is it the best line? When is it going to start making an error? This is the concept of margin, highlited in yellow. Let us draw another line:

![solution](/assets/images/svm_3.svg) 

It seems better than the previous line, since it has wider margin. Let us try to get the best margin:

![solution](/assets/images/svm_4.svg) 

We finally get the widest margin. Can we say that this one is the best boundary line? $E_{\text{in}}$ are all zero, and the generalization performance will be same since we have four points equally. Nonetheless, we may agree with the third one is the best choice, intuitively. Let us now consider two quenstions. 

<br>

<div align="center">

Why is bigger margin better?

<br>

<br>

Which $\mathbf{w}$ maximizes the margin?

</div>

<br>

##### 1.1. Why is bigger margin better? 

Consider we are allowed to use a classifeir, but we have to have at least some margin to accept it. The situation is illustrated in the following figure. 

![solution](/assets/images/svm_5.svg) 

If the least margin should be greater than case (b), we are putting a restriction on the [growth function](https://isopink.github.io/Effective-number-of-hypothesis/). Fat margins imply fewer [dichotomies](https://isopink.github.io/Effective-number-of-hypothesis/) possible. Therefore, we have smaller [VC dimension](https://isopink.github.io/VC-Dimension/), than if we didn't restrict them. 

Although this is informal, we will come to a result that estimates the $E_{\text{out}}$ based on the margin, at the end of this post. And we will find out that indeed, bigger margin enables better $E_{\text{out}}$. So, we now try to believe that the fat margins are good. Let us solve for them. 

<br>

##### 1.2. Which $\mathbf{w}$ maximizes the margin?

Let $x_n$ be the nearest data point to the plane (or hyperplane) $\mathbf{w}^\top \mathbf{x} = 0$. The distance from $x_n$ to the plane will be the margin. Now, there are two prelimaries that we are going to invoke here. They will simplify the analysis later on. First, normalize $\mathbf{w}$ as below: 

<br>

$$
\left| \mathbf{w}^\top \mathbf{x}_n \right| = 1
$$

<br>

Even if we scale $\mathbf{w}$, the plane remains unchanged and the margin is scale invariace. In order to simplify the analysis later, we can normalize $\mathbf{w}$, without loss of generality. Second, pull out $w_0$: 

<br>

$$
\mathbf{w} = (w_1, \cdots, w_d)
$$

<br>

So far, we are adding $w_0$ for the nice representation of vector and matrix. However, when we solove for the margin, it is no longer convenient. So, for the analysis of support vector machines, we are going to pull $w_0$ out. And in order not to confuse it, we are going to call $w_0$ here $b$, for bias. Then the equation for the plane becomes: 

<br>

$$
\mathbf{w}^\top \mathbf{x} + b = 0
$$

<br>

Remember, there is no $x_0$ in the vector $\mathbf{x}$ here. We will keep using this convention that will make our math more friendly. Again, we want to compute the distance from $x_n$ to the plane. Let us look at the geometry of the situation. 

![solution](/assets/images/svm_6.svg) 

On the figure above, the vector $\mathbf{w}$ is perpendicular to the plane in the input space $\mathcal{X}. Let $\mathbf{x}'$ and $\mathbf{x}''$ is on the plane. Then, 

<br>

$$
\mathbf{w}^\top \mathbf{x}' + b = 0 \quad \text{and} \quad \mathbf{w}^\top \mathbf{x}'' + b = 0
$$

<br>

If we take the difference between these two equations: 

<br>

$$
\mathbf{w}^\top (\mathbf{x}' - \mathbf{x}'') = 0
$$

<br>

Since we did not make any restrictions on $\mathbf{x}'$ and $\mathbf{x}''$, vector $\mathbf{w} must be orthogonal to every vector on the plane. Now we can get the distance. 

![solution](/assets/images/svm_7.svg) 

First, pick any point on the plane. We just call it generic $\mathbf{x}$. Then, project the vector from $x$ to $x_n$ on $\mathbf{w}. The distance can be described as:

<br>

$$
\left| \hat{\mathbf{w}}^\top (\mathbf{x}_n - \mathbf{x}) \right|, \quad
\text{where } \hat{\mathbf{w}} = \frac{\mathbf{w}}{\|\mathbf{w}\|}
$$

<br>

This can be simplifeid if we add the missing term bias as: 

<br>

$$
\frac{1}{\|\mathbf{w}\|} \left| \mathbf{w}^\top \mathbf{x}_n + b - \mathbf{w}^\top \mathbf{x} - b \right| = \frac{1}{\|\mathbf{w}\|}
$$

<br>

where $\mathbf{w}^\top \mathbf{x} + b = 0$ as we mentioned before. Let us now formulate the problem. 
