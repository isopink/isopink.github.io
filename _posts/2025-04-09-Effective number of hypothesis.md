---
layout: single
title: "Lecture 5 : Effective number of hypothesis"
---


In this time, we will talk about how to reduce the number of hypothesis, $M$ which we discussed in [<u>Lecture 2</u>.](https://isopink.github.io/Is-Learning-Feasible/)  The discussion will proceed in the following three parts :


1. Where did the $M$ come from? 

2. Ilustrative examples 

3. Break point 

---

#### 1. Where did the $M$ come from? 

At the end of the sesiion, I promised to make $M$ tighter. This is the right time to do that:

<br>

$$
\mathbb{P}\left[ \lvert E_{\text{in}} - E_{\text{out}} \rvert > \epsilon \right] \leq 2M e^{-2\epsilon^2 N}
$$

<br>

This is the inequality we discussed in Lecture 2. The problem is $M$ is almost infinity in most cases, the inequality becomes useless. Before findnig good replacement of $M$, See where dose $M$ come from. 

In Hoeffding’s Inequality, the probability $\mathbb{P}$ refers to the **bad event**. This bad event is defined as the case where the difference between the in-sample error and out-of-sample error exceeds $\epsilon$ for a given hypothesis. Now, because we want this guarantee to hold **for all $M$ possible hypotheses**, we had to consider the worst-case scenario —  that is, assume that **each bad event is mutually exclusive**. 

Under this assumption, we simply added the individual probabilities of each bad event, which is how the factor $M$ ended up in the inequality. **But, in real, These hypotheses might not be mutually exclusive.** The bad events are very overlapping.  

![solution](/assets/images/enh_1.svg)

To make it easier to understand, Let me introduce the classificaition example. Here's the figure of misclassfied hypothesis. :

![solution](/assets/images/enh_2.svg)

Now, Here's the figure of slightly tuned: 

![solution](/assets/images/enh_3.svg)

The highlighted area is $\Delta E_{\text{out}}$ and $\Delta E_{\text{in}}$. However, this area is extremely small. That is to say: 

<br>

$$
\left| E_{\text{in}}(h_1) - E_{\text{out}}(h_1) \right| \approx \left| E_{\text{in}}(h_2) - E_{\text{out}}(h_2) \right|
$$

<br>

You can easily regonize that applying Hoeffding’s Inequality to all hypotheses using the union bound might seem unreasonable. What can we replace $M$ with? It is the **Growth function**. Before introducing the growth function, we need to understand **Dichotomy**.  

![solution](/assets/images/enh_4.svg)

**Let me introduce the key idea of dichotomy first**. As you already know, we cannot measure the out-of-sample error precisely and directly. Instead, we can only measure the in-sample error. For that reason, we will focus only on the behavior of the in-sample error. The entire input space is continuous. However, since we cannot examine every point in that space, we will perform our calculations based on a finite set of sample points. In other words, rather than analyzing how the entire space is divided, we look at how the specific points we selected are classified. **Now, Let's define Dichotomy**. Let $\mathbf{x}_1, \cdots, \mathbf{x}_N \in \mathcal{X}$. The dichotomies generated by $\mathcal{H}$ on these points are defined by: 

<br>

$$
\mathcal{H}(\mathbf{x}_1, \cdots, \mathbf{x}_N) = \left\{ \left(h(\mathbf{x}_1), \cdots, h(\mathbf{x}_N)\right) \mid h \in \mathcal{H} \right\}.
$$

<br>

We can think of the dichotomies 

<br>

$$
m_{\mathcal{H}}(N) = \max_{\mathbf{x}_1, \cdots, \mathbf{x}_N \in \mathcal{X}} \left| \mathcal{H}(\mathbf{x}_1, \cdots, \mathbf{x}_N) \right|
$$

<br>

For any 


<br>

$$
m_{\mathcal{H}}(N) \leq 2^N
$$

<br>

Let's look at a few simple examples to understand how to compute the *Growth Function*. We will compute the *Growth Function* in the context of the 2D Perceptron problem.

![solution](/assets/images/enh_5.svg) 

what is $m_{\mathcal{H}}(3)$ figure on the left shows a dichotomy on $3$ points that the perceptron cannot generate, while figure on the right shows another $3$ points that the perceptron can shatter, generating all $2^3 = 8$ dichotomies. Because the definition of $m_{\mathcal{H}}(N)$ is based on the maximum number of dichotomies, $m_{\mathcal{H}}(3) = 8$ in spite of the case in Figure on the left. 

![solution](/assets/images/enh_6.svg) 

In the case of $4$ points, figure shows a dichotomy that the perceptron cannot generate. you can easily verify that there are no $4$ points that the perceptron can shatter. The most a perceptron can do on any $4$ points is $14$ dichotomies out of the possible $16$, Hence, $m_{\mathcal{H}}(4) = 14$.

---

#### 2. Illustrative examples 

Let us now illustrate how to compute $m_{\mathcal{H}(N)}$ for some simple hypothesis sets. 



##### 2.1. Positive rays

$\mathcal{H}$ consists of all hypotheses $h: \mathbb{R} \rightarrow \pm 1$ of the form $h(x) = \text{sign}(x - a)$. They return $-1$ to the left of some value $a$ and $+1$ to the right of $a$.

![solution](/assets/images/enh_7.svg) 

The dichotomy we get on the $N$ points is decided by which region contains the value $a$. As we vary $a$, we will get $N+1$ different dichotomies. Since this is t he most we can get for any $N$ points, the growth function is : 

<br>

$$ m_{\mathcal{H}(N)} = N + 1 $$

<br>

##### 2.2 Positive intervals

$\mathcal{H}$ is set of $h: \mathbb{R} \rightarrow \pm 1$. It returns $+1$ within some interval and $-1$ otherwise. Each hypothesis is specified by the two end values of that interval. 

![solution](/assets/images/enh_8.svg) 

To compute $m_{\mathcal{H}}(N)$, we notice that given $N$ points, the line is again split by the points into $N + 1$ regions. The dichotomy we get is decided by which two regions contain the end values of the interval. If both end values fall in the same region, the resulting hypothesis is the constant $-1$ regardless of which region it is. Adding up these possibilities, we get:

<br>

$$
m_{\mathcal{H}}(N) = \binom{N+1}{2} + 1 
$$

<br>

##### 2.3 Convex Sets

Convex sets are $\mathcal{H}$ consists of all hypotheses in two dimensions $h: \mathbb{R}^2 \rightarrow \pm 1$ that are positive inside some convex set and negative elsewhere.

![solution](/assets/images/enh_9.svg)

To compute $m_{\mathcal{H}}(N)$ in this case, we need to choose the $N$ points carefully. Assume that $N$ points are on the perimeter of a circle. Now consider any dichotomy on these points, assigning an arbitrary pattern of $\pm1$'s to the $N$ points. They are all convex sets. This means that any dichotomy on these $N$ points can be realized using a convex hypothesis, so $\mathcal{H}$ manages to shatter these points and the growth function has the maximum possible value : 

<br>

$$
m_{\mathcal{H}}(N) = 2^N.
$$

<br>

So, far we calculated very well, but it is not practical to try to compute $m_{\mathcal{H}}(N)$ for every hypothesis set we use. Fortunately, we don't have to. Since $m_{\mathcal{H}}(N)$ is meant to replace $M$ in next chapter. we can use an upper bound on $m_{\mathcal{H}}(N)$ instead of the exact value, and the Hoeffidng inequality will still hold. Getting a good bound on $m_{\mathcal{H}}(N)$ will prove much easier than computing $m_{\mathcal{H}}(N)$ itself. To do so, let us introduce the **break point.**

---

#### 3. Break point 


If no data set of size $k$ can be shattered by $\mathcal{H}$, then $k$ is said to be a break point for $\mathcal{H}$. If $k$ is a break point, then $m_{\mathcal{H}}(k) < 2^k$. Out previous example shows that $k = 4$ is a break point for two-dimensional perceptrons.

In the case of Positive Rays, we can see that the smallest value of $N$ satisfying $m_{\mathcal{H}}(N) = N + 1 < 2^N$ is $N = 2$. Therefore, the break point is $k = 2$. 

Likewise, for Positive Intervals, we find that $m_{\mathcal{H}}(2) = 4$, $m_{\mathcal{H}}(3) = 7 < 2^3 = 8$, so it is easy to compute that the break point is $k = 3$. 

However, in the case of Convex Sets, we have $m_{\mathcal{H}}(N) = 2^N$, which means no value of $k$ satisfies the break point condition. Thus, we cannot find a break point for this hypothesis set.

Based on these observations, **if a break point exists, we can expect that the upper bound of the growth function will be significantly reduced**. In other words : 

<br>

<p align="center">
No break point ⟹ $m_{\mathcal{H}}(N) = 2^N$
</p>

<p align="center">
Any break point ⟹ $m_{\mathcal{H}}(N)$ is <b>polynomial</b> in $N$
</p>

<br>
